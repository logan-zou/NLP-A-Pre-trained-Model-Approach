{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实现《自然语言处理——预训练模型方法》第四章代码上（4.1~4.5）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. MLP实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入所需库\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个MLP类\n",
    "class MLP(nn.Module):\n",
    "    # 基类为nn.Module\n",
    "    def __init__(self, input_dim, hidden_dim, num_class):\n",
    "        # 构造函数\n",
    "        # input_dim:输入数据维度\n",
    "        # hidden_dim:隐藏层维度\n",
    "        # num_class:多分类个数\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        self.linear1 = nn.Linear(input_dim, hidden_dim)\n",
    "        # 隐含层，线性变换\n",
    "        self.activate = F.relu\n",
    "        # 使用relu函数作为激活函数：小于0的值输出为0\n",
    "        self.linear2 = nn.Linear(hidden_dim, num_class)\n",
    "        # 输出层，线性变换\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # 前向计算函数\n",
    "        # inputs:输入\n",
    "        print(f\"输入为：{inputs}\")\n",
    "        hidden = self.linear1(inputs)\n",
    "        print(f\"经过隐含层变换为：{hidden}\")\n",
    "        activation = self.activate(hidden)\n",
    "        print(f\"经过激活后为：{activation}\")\n",
    "        outputs = self.linear2(activation)\n",
    "        print(f\"输出层输出为：{outputs}\")\n",
    "        probs = F.softmax(outputs, dim = 1)\n",
    "        print(f\"输出概率值为：{probs}\")\n",
    "        # 归一化为概率值\n",
    "        return probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入为：tensor([[0.2840, 0.3345, 0.5389, 0.4109],\n",
      "        [0.1151, 0.6950, 0.7657, 0.8501],\n",
      "        [0.1254, 0.6561, 0.3595, 0.6281]])\n",
      "经过隐含层变换为：tensor([[ 0.0918, -0.3648,  0.0221, -0.0366,  0.2924],\n",
      "        [ 0.0231, -0.3179, -0.2034,  0.1455,  0.2739],\n",
      "        [-0.0328, -0.2742,  0.0010,  0.0370,  0.2395]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "经过激活后为：tensor([[0.0918, 0.0000, 0.0221, 0.0000, 0.2924],\n",
      "        [0.0231, 0.0000, 0.0000, 0.1455, 0.2739],\n",
      "        [0.0000, 0.0000, 0.0010, 0.0370, 0.2395]], grad_fn=<ReluBackward0>)\n",
      "输出层输出为：tensor([[0.4405, 0.1669],\n",
      "        [0.5124, 0.1831],\n",
      "        [0.4834, 0.1595]], grad_fn=<AddmmBackward0>)\n",
      "输出概率值为：tensor([[0.5680, 0.4320],\n",
      "        [0.5816, 0.4184],\n",
      "        [0.5803, 0.4197]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.5680, 0.4320],\n",
      "        [0.5816, 0.4184],\n",
      "        [0.5803, 0.4197]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 调用该模型\n",
    "mlp = MLP(input_dim = 4, hidden_dim = 5, num_class = 2)\n",
    "inputs = torch.rand(3, 4)\n",
    "probs = mlp(inputs)\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 卷积神经网络实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入所需库\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn import Conv1d\n",
    "from torch.nn import MaxPool1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个CNN类\n",
    "# 注：该类为笔者自行实现\n",
    "class CNN(nn.Module):\n",
    "    # 基类为nn.Module\n",
    "    def __init__(self, input_dim, output_dim, num_class, kernel_size):\n",
    "        # 构造函数\n",
    "        # input_dim:输入数据维度\n",
    "        # output_dim:卷积输出维度\n",
    "        # num_class:多分类个数\n",
    "        # kernel_size：卷积核宽度\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.conv = Conv1d(input_dim, output_dim, kernel_size)\n",
    "        # 卷积层\n",
    "        self.pool = F.max_pool1d\n",
    "        # 池化层，使用最大池化\n",
    "        self.linear = nn.Linear(output_dim, num_class)\n",
    "        # 输出层，线性变换\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # 前向计算函数\n",
    "        # inputs:输入\n",
    "        print(f\"输入size为：{inputs.size()}\")\n",
    "        conv = self.conv(inputs)\n",
    "        print(f\"经过卷积层变换size为：{conv.size()}\")\n",
    "        pool = self.pool(conv, kernel_size = conv.shape[2])\n",
    "        print(f\"经过池化后size为：{pool.size()}\")\n",
    "        pool_squeeze = pool.squeeze(dim=2)\n",
    "        outputs = self.linear(pool_squeeze)\n",
    "        print(f\"输出层输出size为：{outputs.size()}\")\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入size为：torch.Size([2, 5, 6])\n",
      "经过隐含层变换size为：torch.Size([2, 2, 3])\n",
      "经过池化后size为：torch.Size([2, 2, 1])\n",
      "输出层输出size为：torch.Size([2, 2])\n",
      "tensor([[0.1097, 0.4246],\n",
      "        [0.0275, 0.3587]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 调用该模型\n",
    "cnn = CNN(5, 2, 2, 4)\n",
    "inputs = torch.rand(2,5,6)\n",
    "probs = cnn(inputs)\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 循环神经网络实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入所需库\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn import RNN\n",
    "from torch.nn import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN可直接调用函数构造\n",
    "rnn = RNN(input_size = 4, hidden_size = 5, batch_first = True)\n",
    "# 定义一个输入为4维，输出为5维的RNN网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.rand(2,3,4)\n",
    "# 批次为2，序列长度为3，输入维度为4\n",
    "outputs, hn = rnn(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2286,  0.8542,  0.4634, -0.4545,  0.8325],\n",
       "         [ 0.5414,  0.8282,  0.7302, -0.6027,  0.8845],\n",
       "         [ 0.5205,  0.9020,  0.7551, -0.6717,  0.8920]],\n",
       "\n",
       "        [[ 0.3798,  0.7895,  0.4614, -0.3672,  0.7947],\n",
       "         [ 0.2242,  0.8136,  0.7505, -0.6971,  0.8387],\n",
       "         [ 0.7363,  0.9302,  0.7960, -0.6228,  0.9013]]],\n",
       "       grad_fn=<TransposeBackward1>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs\n",
    "# outputs为所有状态的输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.5205,  0.9020,  0.7551, -0.6717,  0.8920],\n",
       "         [ 0.7363,  0.9302,  0.7960, -0.6228,  0.9013]]],\n",
       "       grad_fn=<StackBackward0>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hn\n",
    "# hn为最后一个时刻隐藏层的输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 修改参数构建rnn\n",
    "\n",
    "new_rnn = RNN(input_size = 4, hidden_size = 5, batch_first = True, bidirectional = False, num_layers = 3)\n",
    "# bidirectional——是否双向循环\n",
    "# num_layer——隐含层层数\n",
    "outputs, hn = new_rnn(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.4804, -0.3505, -0.1715,  0.4986,  0.1992],\n",
       "         [ 0.1356, -0.6146, -0.1079,  0.5493, -0.1920],\n",
       "         [ 0.0440, -0.5984,  0.0594,  0.6036, -0.3099]],\n",
       "\n",
       "        [[ 0.4907, -0.4173, -0.1886,  0.4708,  0.1714],\n",
       "         [ 0.0991, -0.6351, -0.1140,  0.5796, -0.2207],\n",
       "         [ 0.0855, -0.5836,  0.0795,  0.5775, -0.3130]]],\n",
       "       grad_fn=<TransposeBackward1>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0713, -0.6564, -0.5520, -0.2442,  0.0394],\n",
       "         [ 0.0119, -0.4096, -0.6544, -0.2719,  0.0766]],\n",
       "\n",
       "        [[-0.4639, -0.4273,  0.4186, -0.8699, -0.4914],\n",
       "         [-0.5272, -0.3858,  0.3944, -0.8775, -0.4276]],\n",
       "\n",
       "        [[ 0.0440, -0.5984,  0.0594,  0.6036, -0.3099],\n",
       "         [ 0.0855, -0.5836,  0.0795,  0.5775, -0.3130]]],\n",
       "       grad_fn=<StackBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM也可直接调用函数构造\n",
    "lstm = LSTM(input_size = 4, hidden_size = 5, batch_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs, (hn, cn) = lstm(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0135,  0.1853,  0.1818, -0.0742, -0.0463],\n",
       "         [ 0.0095,  0.2240,  0.3504,  0.0161, -0.1934],\n",
       "         [-0.0084,  0.2773,  0.3812,  0.0006, -0.2226]],\n",
       "\n",
       "        [[ 0.0207,  0.1564,  0.2159, -0.0242, -0.0964],\n",
       "         [ 0.0205,  0.2207,  0.2431, -0.1132, -0.0822],\n",
       "         [ 0.0554,  0.3030,  0.3500, -0.1545, -0.0900]]],\n",
       "       grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs\n",
    "# 值含义同rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0084,  0.2773,  0.3812,  0.0006, -0.2226],\n",
       "         [ 0.0554,  0.3030,  0.3500, -0.1545, -0.0900]]],\n",
       "       grad_fn=<StackBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hn\n",
    "# 值含义同rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0177,  0.5129,  0.8251,  0.0011, -0.4486],\n",
       "         [ 0.0943,  0.5583,  0.8232, -0.2650, -0.1795]]],\n",
       "       grad_fn=<StackBackward0>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cn\n",
    "# cn为最后一个时刻记忆细胞的值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 注意力网络实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入所需库\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 搭建一个Encoder层\n",
    "encoder_layer = nn.TransformerEncoderLayer(d_model=4, nhead=2)\n",
    "# 输入、输出维度为4，头数为2\n",
    "encoder = nn.TransformerEncoder(encoder_layer, num_layers=5)\n",
    "outputs = encoder(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1555,  1.6035, -1.1460, -0.3020],\n",
       "         [-0.7393,  1.5755, -0.9756,  0.1395],\n",
       "         [ 0.1729,  0.6933,  0.8147, -1.6809]],\n",
       "\n",
       "        [[-0.8905,  1.2357, -1.0727,  0.7276],\n",
       "         [ 1.6451, -0.4516, -0.1625, -1.0311],\n",
       "         [-0.8592,  0.5700, -1.0586,  1.3477]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 搭建一个Decoder层\n",
    "decoder_layer = nn.TransformerDecoderLayer(d_model=4, nhead=2)\n",
    "decoder = nn.TransformerDecoder(decoder_layer, num_layers=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_another = torch.rand(2,3,4)\n",
    "final_outputs = decoder(inputs_another, outputs)\n",
    "# inputs_another是当前输入，outputs是解码的输出作为历史记忆"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1466,  1.5527, -0.6007, -1.0985],\n",
       "         [ 0.4964, -1.6062,  0.0267,  1.0831],\n",
       "         [ 0.0523,  1.4473, -0.1259, -1.3736]],\n",
       "\n",
       "        [[ 0.0510,  1.5564, -0.4276, -1.1798],\n",
       "         [ 0.0870, -1.5946,  0.3580,  1.1496],\n",
       "         [-1.1281,  0.9141,  1.0737, -0.8596]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. 梯度下降法训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入所需库\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个使用logSoftmax操作的MLP类\n",
    "class MLP(nn.Module):\n",
    "    # 基类为nn.Module\n",
    "    def __init__(self, input_dim, hidden_dim, num_class):\n",
    "        # 构造函数\n",
    "        # input_dim:输入数据维度\n",
    "        # hidden_dim:隐藏层维度\n",
    "        # num_class:多分类个数\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        self.linear1 = nn.Linear(input_dim, hidden_dim)\n",
    "        # 隐含层，线性变换\n",
    "        self.activate = F.relu\n",
    "        # 使用relu函数作为激活函数：小于0的值输出为0\n",
    "        self.linear2 = nn.Linear(hidden_dim, num_class)\n",
    "        # 输出层，线性变换\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # 前向计算函数\n",
    "        # inputs:输入\n",
    "        # print(f\"输入为：{inputs}\")\n",
    "        hidden = self.linear1(inputs)\n",
    "        # print(f\"经过隐含层变换为：{hidden}\")\n",
    "        activation = self.activate(hidden)\n",
    "        # print(f\"经过激活后为：{activation}\")\n",
    "        outputs = self.linear2(activation)\n",
    "        # print(f\"输出层输出为：{outputs}\")\n",
    "        probs = F.log_softmax(outputs, dim = 1)\n",
    "        # print(f\"输出概率值为：{probs}\")\n",
    "        # 归一化为概率值\n",
    "        return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建异或问题的输入输出\n",
    "train_data = torch.tensor([[0.0,0.0], [0.0,1.0], [1.0,0.0], [1.0,1.0]])\n",
    "train_label = torch.tensor([0,1,1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建MLP模型\n",
    "mlp = MLP(input_dim=2, hidden_dim=5, num_class=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 观察未训练的输出\n",
    "y_pre = mlp(train_data)\n",
    "y_pre.argmax(dim = 1)\n",
    "# 取预测最大值作为类别结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "# 使用负对数似然损失\n",
    "optimizer = optim.SGD(mlp.parameters(), lr=0.05)\n",
    "# 使用梯度下降法\n",
    "for epoch in range(500):\n",
    "    y = mlp(train_data)\n",
    "    loss = criterion(y, train_label)\n",
    "    optimizer.zero_grad()\n",
    "    # 将优化器的梯度置零\n",
    "    loss.backward()\n",
    "    # 梯度的反向传播\n",
    "    optimizer.step()\n",
    "    # 更新参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear1.weight tensor([[ 0.7744,  0.6532],\n",
      "        [-0.0648, -0.3256],\n",
      "        [-0.1493, -0.2856],\n",
      "        [ 0.4949, -0.1721],\n",
      "        [-1.2461, -1.2449]])\n",
      "linear1.bias tensor([-0.2786,  0.7907,  0.4438, -0.4979,  1.2473])\n",
      "linear2.weight tensor([[ 0.5600, -0.2013, -0.5235,  0.3350,  1.5162],\n",
      "        [-0.6237,  0.6042,  0.1783,  0.0218, -1.3404]])\n",
      "linear2.bias tensor([-0.4435,  0.2404])\n"
     ]
    }
   ],
   "source": [
    "# 打印训练后的参数\n",
    "for name, param in mlp.named_parameters():\n",
    "    print(name, param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 1, 0])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pre = mlp(train_data)\n",
    "y_pre.argmax(dim = 1)\n",
    "# 取预测最大值作为类别结果"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('env_for_gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b125423cf5c6421dfa630e1e0530026ff8478baf79673e49ec884511905104d1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
