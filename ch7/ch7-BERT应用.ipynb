{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æƒ…æ„Ÿåˆ†ç±»ä»»åŠ¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zouyuheng/.conda/envs/zyh_pytorch/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datasets import load_dataset, load_metric\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (/home/zouyuheng/.cache/huggingface/datasets/glue/sst2/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 648.74it/s]\n"
     ]
    }
   ],
   "source": [
    "# åŠ è½½è®­ç»ƒæ•°æ®\n",
    "dataset = load_dataset('glue', 'sst2')\n",
    "# æ­¤å‡½æ•°ç›´æ¥ä»ç½‘ç»œä¸‹è½½åŸºå‡†æ•°æ®é›†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_415245/3104039380.py:6: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ğŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric('/home/zouyuheng/tool/huggingface-datasets/glue.py', 'sst2')\n"
     ]
    }
   ],
   "source": [
    "# åŠ è½½åˆ†è¯å™¨ã€é¢„è®­ç»ƒæ¨¡å‹å’Œè¯„ä»·æ–¹æ³•\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')\n",
    "# æ­¤å¤„çš„å‚æ•°å¯ä»¥é€‰æ‹©ä¸åŒçš„åˆ†è¯å™¨ï¼Œå¯ä»¥åœ¨hagging face å®˜ç½‘æŸ¥é˜…\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-cased', return_dict = True)\n",
    "# æ¨¡å‹é€‰æ‹©åŒç†ï¼Œè¿˜å¯ä»¥é€‰æ‹©ä¸€äº›å­¦è€…ä¸Šä¼ çš„ã€ç‰¹åˆ«çš„é¢„è®­ç»ƒæ¨¡å‹\n",
    "metric = load_metric('/home/zouyuheng/tool/huggingface-datasets/glue.py', 'sst2')\n",
    "# åŸºå‡†æ•°æ®é›†æ˜¯æœ‰ç‰¹å®šçš„è¯„ä»·æŒ‡æ ‡çš„\n",
    "# æ³¨æ„ï¼Œæ­¤å¤„merticçš„åŠ è½½æ— æ³•ç›´æ¥è”ç½‘è·å–ï¼Œéœ€è¦å°†å¯¹åº”æ–‡ä»¶ä¸‹è½½åˆ°æœ¬åœ°ï¼Œè¾“å…¥æœ¬åœ°æ–‡ä»¶è·¯å¾„è·å–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function tokenize at 0x7f630369a170> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 67/68 [00:14<00:00,  4.54ba/s]\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:00<00:00,  2.68ba/s]\n",
      " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 67/68 [00:40<00:00,  1.66ba/s]\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:01<00:01,  1.05s/ba]\n"
     ]
    }
   ],
   "source": [
    "# å¯¹è®­ç»ƒé›†åˆ†è¯\n",
    "def tokenize(examples):\n",
    "    return tokenizer(examples['sentence'], truncation = True, padding = 'max_length')\n",
    "    # æœªæŸ¥è¯¢åˆ°è¿™ä¸¤ä¸ªå‚æ•°çš„APIï¼ŒçŒœæµ‹truncationæŒ‡æ˜å¯¹äºè¿‡é•¿åºåˆ—æ˜¯å¦é˜¶æ®µï¼Œpaddingåˆ™æ˜¯æŒ‡å¯¹äºä¸åˆ°æœ€å¤§é•¿åº¦çš„åºåˆ—è¿›è¡Œpadæ“ä½œ\n",
    "dataset = dataset.map(tokenize, batched = True)\n",
    "# é€šè¿‡tokenizeå¯¹æ•°æ®é›†è¿›è¡Œæ‰¹é‡å¤„ç†\n",
    "encoded_dataset = dataset.map(lambda example: {'label': example['label']}, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 67349\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 872\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1821\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 67349\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 872\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1821\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# æ•°æ®æ ¼å¼è½¬æ¢\n",
    "columns = ['input_ids', 'token_type_ids', 'attention_mask', 'label']\n",
    "encoded_dataset.set_format(type = 'torch',  columns = columns)\n",
    "encoded_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®šä¹‰è¯„ä»·æŒ‡æ ‡\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    return metric.compute(predictions = np.argmax(predictions, axis = 1), references = labels)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®¾ç½®è®­ç»ƒè¶…å‚\n",
    "args = TrainingArguments(\n",
    "    \"ft-ss2\", # è¾“å‡ºè·¯å¾„\n",
    "    evaluation_strategy = \"epoch\",# æ¯è½®ç»“æŸåè¿›è¡Œè¯„ä»·\n",
    "    learning_rate = 2e-5,\n",
    "    per_device_train_batch_size = 32,# è®­ç»ƒæ‰¹æ¬¡å¤§å°\n",
    "    per_device_eval_batch_size = 32,# éªŒè¯æ‰¹æ¬¡å¤§å°\n",
    "    num_train_epochs = 2 # è®­ç»ƒè½®æ¬¡\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®­ç»ƒ\n",
    "trainer = Trainer(\n",
    "    model, \n",
    "    args, \n",
    "    train_dataset = encoded_dataset[\"train\"],\n",
    "    eval_dataset = encoded_dataset[\"validation\"],\n",
    "    tokenizer = tokenizer,\n",
    "    compute_metrics = compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, idx. If sentence, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/zouyuheng/.conda/envs/zyh_pytorch/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 67349\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 4210\n",
      "  Number of trainable parameters = 108311810\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='139' max='4210' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 139/4210 01:43 < 51:23, 1.32 it/s, Epoch 0.07/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# å¼€å§‹è®­ç»ƒ\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m~/.conda/envs/zyh_pytorch/lib/python3.10/site-packages/transformers/trainer.py:1501\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   1498\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1499\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1500\u001b[0m )\n\u001b[0;32m-> 1501\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1502\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1503\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1504\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1505\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1506\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/zyh_pytorch/lib/python3.10/site-packages/transformers/trainer.py:1749\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1747\u001b[0m         tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1748\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1749\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[1;32m   1751\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1752\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1753\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1754\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1755\u001b[0m ):\n\u001b[1;32m   1756\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1757\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/.conda/envs/zyh_pytorch/lib/python3.10/site-packages/transformers/trainer.py:2526\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2524\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdeepspeed\u001b[39m.\u001b[39mbackward(loss)\n\u001b[1;32m   2525\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 2526\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m   2528\u001b[0m \u001b[39mreturn\u001b[39;00m loss\u001b[39m.\u001b[39mdetach()\n",
      "File \u001b[0;32m~/.conda/envs/zyh_pytorch/lib/python3.10/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/.conda/envs/zyh_pytorch/lib/python3.10/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# å¼€å§‹è®­ç»ƒ\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¿›è¡Œè¯„ä¼°\n",
    "trainer.evaluate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¥å¯¹æ–‡æœ¬åˆ†ç±»ä»»åŠ¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zouyuheng/.conda/envs/zyh_pytorch/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datasets import load_dataset, load_metric\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset glue/rte to /home/zouyuheng/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 697k/697k [00:00<00:00, 786kB/s] \n",
      "                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset glue downloaded and prepared to /home/zouyuheng/.cache/huggingface/datasets/glue/rte/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 694.04it/s]\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_416044/3721628050.py:9: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ğŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric('/home/zouyuheng/tool/huggingface-datasets/glue.py', 'rte')\n"
     ]
    }
   ],
   "source": [
    "# åŠ è½½è®­ç»ƒæ•°æ®\n",
    "dataset = load_dataset('glue', 'rte')\n",
    "# æ­¤å‡½æ•°ç›´æ¥ä»ç½‘ç»œä¸‹è½½åŸºå‡†æ•°æ®é›†\n",
    "# åŠ è½½åˆ†è¯å™¨ã€é¢„è®­ç»ƒæ¨¡å‹å’Œè¯„ä»·æ–¹æ³•\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')\n",
    "# æ­¤å¤„çš„å‚æ•°å¯ä»¥é€‰æ‹©ä¸åŒçš„åˆ†è¯å™¨ï¼Œå¯ä»¥åœ¨hagging face å®˜ç½‘æŸ¥é˜…\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-cased', return_dict = True)\n",
    "# æ¨¡å‹é€‰æ‹©åŒç†ï¼Œè¿˜å¯ä»¥é€‰æ‹©ä¸€äº›å­¦è€…ä¸Šä¼ çš„ã€ç‰¹åˆ«çš„é¢„è®­ç»ƒæ¨¡å‹\n",
    "metric = load_metric('/home/zouyuheng/tool/huggingface-datasets/glue.py', 'rte')\n",
    "# åŸºå‡†æ•°æ®é›†æ˜¯æœ‰ç‰¹å®šçš„è¯„ä»·æŒ‡æ ‡çš„\n",
    "# æ³¨æ„ï¼Œæ­¤å¤„merticçš„åŠ è½½æ— æ³•ç›´æ¥è”ç½‘è·å–ï¼Œéœ€è¦å°†å¯¹åº”æ–‡ä»¶ä¸‹è½½åˆ°æœ¬åœ°ï¼Œè¾“å…¥æœ¬åœ°æ–‡ä»¶è·¯å¾„è·å–\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 2490\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 277\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 3000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:00<00:00,  2.82ba/s]\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:00<00:00,  2.79ba/s]\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:01<00:00,  1.04ba/s]\n",
      "  0%|          | 0/1 [00:00<?, ?ba/s]\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:01<00:00,  1.01ba/s]\n"
     ]
    }
   ],
   "source": [
    "# å¯¹è®­ç»ƒé›†åˆ†è¯\n",
    "def tokenize(examples):\n",
    "    return tokenizer(examples['sentence1'], examples['sentence2'], truncation = True, padding = 'max_length')\n",
    "    # æ³¨æ„ï¼Œç”±äºdatasetç‰ˆæœ¬æ›´æ–°ï¼Œæ­¤å¤„é”®ååº”æ”¹ä¸ºsentence1å’Œsentence2\n",
    "dataset = dataset.map(tokenize, batched = True)\n",
    "# é€šè¿‡tokenizeå¯¹æ•°æ®é›†è¿›è¡Œæ‰¹é‡å¤„ç†\n",
    "encoded_dataset = dataset.map(lambda example: {'label': example['label']}, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ•°æ®æ ¼å¼è½¬æ¢\n",
    "columns = ['input_ids', 'token_type_ids', 'attention_mask', 'label']\n",
    "encoded_dataset.set_format(type = 'torch',  columns = columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®šä¹‰è¯„ä»·æŒ‡æ ‡\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    return metric.compute(predictions = np.argmax(predictions, axis = 1), references = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®¾ç½®è®­ç»ƒè¶…å‚\n",
    "args = TrainingArguments(\n",
    "    \"ft-rte\", # è¾“å‡ºè·¯å¾„\n",
    "    evaluation_strategy = \"epoch\",# æ¯è½®ç»“æŸåè¿›è¡Œè¯„ä»·\n",
    "    learning_rate = 2e-5,\n",
    "    per_device_train_batch_size = 32,# è®­ç»ƒæ‰¹æ¬¡å¤§å°\n",
    "    per_device_eval_batch_size = 32,# éªŒè¯æ‰¹æ¬¡å¤§å°\n",
    "    num_train_epochs = 2 # è®­ç»ƒè½®æ¬¡\n",
    ")\n",
    "\n",
    "# è®­ç»ƒ\n",
    "trainer = Trainer(\n",
    "    model, \n",
    "    args, \n",
    "    train_dataset = encoded_dataset[\"train\"],\n",
    "    eval_dataset = encoded_dataset[\"validation\"],\n",
    "    tokenizer = tokenizer,\n",
    "    compute_metrics = compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æŠ½å–å¼é˜…è¯»ç†è§£ä»»åŠ¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_dataset, load_metric\n",
    "from transformers import BertTokenizerFast, BertForQuestionAnswering, TrainingArguments, Trainer, default_data_collator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset squad (/home/zouyuheng/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 528.75it/s]\n",
      "loading file vocab.txt from cache at /home/zouyuheng/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/vocab.txt\n",
      "loading file tokenizer.json from cache at /home/zouyuheng/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/zouyuheng/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/zouyuheng/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading configuration file config.json from cache at /home/zouyuheng/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/zouyuheng/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# åŠ è½½è®­ç»ƒæ•°æ®\n",
    "dataset = load_dataset('squad')\n",
    "# æ­¤å‡½æ•°ç›´æ¥ä»ç½‘ç»œä¸‹è½½åŸºå‡†æ•°æ®é›†\n",
    "# åŠ è½½åˆ†è¯å™¨ã€é¢„è®­ç»ƒæ¨¡å‹å’Œè¯„ä»·æ–¹æ³•\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')\n",
    "# æ­¤å¤„çš„å‚æ•°å¯ä»¥é€‰æ‹©ä¸åŒçš„åˆ†è¯å™¨ï¼Œå¯ä»¥åœ¨hagging face å®˜ç½‘æŸ¥é˜…\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-cased', return_dict = True)\n",
    "# æ¨¡å‹é€‰æ‹©åŒç†ï¼Œè¿˜å¯ä»¥é€‰æ‹©ä¸€äº›å­¦è€…ä¸Šä¼ çš„ã€ç‰¹åˆ«çš„é¢„è®­ç»ƒæ¨¡å‹\n",
    "metric = load_metric('/home/zouyuheng/tool/huggingface-datasets/squad.py')\n",
    "# åŸºå‡†æ•°æ®é›†æ˜¯æœ‰ç‰¹å®šçš„è¯„ä»·æŒ‡æ ‡çš„\n",
    "# æ³¨æ„ï¼Œæ­¤å¤„merticçš„åŠ è½½æ— æ³•ç›´æ¥è”ç½‘è·å–ï¼Œéœ€è¦å°†å¯¹åº”æ–‡ä»¶ä¸‹è½½åˆ°æœ¬åœ°ï¼Œè¾“å…¥æœ¬åœ°æ–‡ä»¶è·¯å¾„è·å–\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å‡†å¤‡è®­ç»ƒæ•°æ®\n",
    "'''è¿™ä¸€éƒ¨åˆ†æ²¡æœ‰å®Œå…¨ç†è§£ï¼Œè¯¥éƒ¨åˆ†ä½¿ç”¨äº†è¾ƒå¤šä¸“ç”¨APIï¼Œç”±äºæœ¬äººå¹¶ä¸ä¸“æ”»é˜…è¯»ç†è§£ä»»åŠ¡ï¼Œæš‚ä¸æ·±å…¥æ€è€ƒ'''\n",
    "def prepare_train_features(examples):\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples['question'], # é—®é¢˜æ–‡æœ¬\n",
    "        examples['context'], # ç¯‡ç« æ–‡æœ¬\n",
    "        truncation = 'only_second', # æˆªæ–­åªå‘ç”Ÿåœ¨ç¬¬äºŒéƒ¨åˆ†ï¼Œå³åªæˆªæ–­ç¯‡ç« æ–‡æœ¬\n",
    "        max_length = 384, # æœ€å¤§é•¿åº¦\n",
    "        stride = 128, # ç¯‡ç« åˆ‡ç‰‡æ­¥é•¿\n",
    "        return_overflowing_tokens = True, # è¿”å›è¶…å‡ºæœ€å¤§é•¿åº¦çš„æ ‡è®°ï¼Œå°†ç¯‡ç« åˆ‡æˆå¤šç‰‡\n",
    "        return_offsets_mapping = True, # è¿”å›åç½®ä¿¡æ¯ï¼Œç”¨äºå¯¹é½ç­”æ¡ˆä½ç½®\n",
    "        padding = 'max_length' # æŒ‰æœ€å¤§é•¿åº¦è¡¥é½\n",
    "    )\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    # å»ºç«‹åˆ°exampleçš„æ˜ å°„å…³ç³»\n",
    "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "    # å»ºç«‹tokenåˆ°åŸæ–‡çš„å­—ç¬¦çº§æ˜ å°„å…³ç³»ï¼Œç”¨äºç¡®å®šç­”æ¡ˆçš„å¼€å§‹ä½ç½®å’Œç»“æŸä½ç½®\n",
    "\n",
    "    tokenized_examples[\"start_position\"] = []\n",
    "    tokenized_examples[\"end_position\"] = []\n",
    "    # å¼€å§‹ä½ç½®å’Œç»“æŸä½ç½®\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        # éå†è¾“å…¥åºåˆ—\n",
    "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "        # è·å–è¾“å…¥åºåˆ—çš„input_idsä»¥åŠ[cls]æ ‡è®°çš„ä½ç½®\n",
    "\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "        # è·å–å“ªäº›éƒ¨åˆ†æ˜¯é—®é¢˜ï¼Œå“ªäº›éƒ¨åˆ†æ˜¯ç¯‡ç« \n",
    "\n",
    "        sample_index = sample_mapping[i]\n",
    "        # ç¬¬iä¸ªåºåˆ—\n",
    "        answers = examples[\"answers\"][sample_index]\n",
    "        # ç­”æ¡ˆ\n",
    "        start_char = answers[\"answer_start\"][0]\n",
    "        end_char = start_char + len(answers[\"text\"][0])\n",
    "        # ç­”æ¡ˆçš„å¼€å§‹ç»“æŸä½ç½®\n",
    "\n",
    "        token_start_index = 0\n",
    "        while sequence_ids[token_start_index] != 1:\n",
    "            token_start_index += 1\n",
    "        # æ‰¾åˆ°å€¼ä¸º1çš„ä½ç½®\n",
    "        token_end_index = 0\n",
    "        while sequence_ids[token_end_index] != 1:\n",
    "            token_end_index += 1\n",
    "        # åŒä¸Šï¼Œåº”è¯¥æ˜¯å€¼ä¸º1æ ‡å¿—ç€ç­”æ¡ˆå¼€å§‹å’Œç»“æŸ\n",
    "\n",
    "        if not(offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "            # ç­”æ¡ˆæ˜¯å¦è¶…å‡ºå½“å‰åˆ‡ç‰‡çš„èŒƒå›´\n",
    "            tokenized_examples[\"start_position\"].append(cls_index)\n",
    "            tokenized_examples[\"end_position\"].append(cls_index)\n",
    "            # å¦‚æœè¶…å‡ºï¼Œå¼€å§‹å’Œç»“æŸçš„ä½ç½®å‡è®¾ç½®ä¸ºclsçš„ä½ç½®\n",
    "        else:\n",
    "            while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                token_start_index +=1\n",
    "            tokenized_examples[\"start_position\"].append(token_start_index - 1)\n",
    "            while offsets[token_end_index][1] >= end_char:\n",
    "                token_end_index -= 1\n",
    "            tokenized_examples[\"end_position\"].append(token_end_index + 1)\n",
    "            # å°†å¼€å§‹å’Œç»“æŸä½ç½®ç§»è‡³ç¯‡ç« ä¸­ç­”æ¡ˆçš„ä¸¤ç«¯\n",
    "\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 87/88 [00:31<00:00,  2.73ba/s]\n",
      " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 10/11 [00:04<00:00,  2.33ba/s]\n"
     ]
    }
   ],
   "source": [
    "tokenize_datasets = dataset.map(prepare_train_features, batched=True, remove_columns=dataset[\"train\"].column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "# è®¾ç½®è®­ç»ƒè¶…å‚\n",
    "args = TrainingArguments(\n",
    "    \"ft-squad\", # è¾“å‡ºè·¯å¾„\n",
    "    evaluation_strategy = \"epoch\",# æ¯è½®ç»“æŸåè¿›è¡Œè¯„ä»·\n",
    "    learning_rate = 2e-5,\n",
    "    per_device_train_batch_size = 32,# è®­ç»ƒæ‰¹æ¬¡å¤§å°\n",
    "    per_device_eval_batch_size = 32,# éªŒè¯æ‰¹æ¬¡å¤§å°\n",
    "    num_train_epochs = 2 # è®­ç»ƒè½®æ¬¡\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®­ç»ƒ\n",
    "trainer = Trainer(\n",
    "    model, \n",
    "    args, \n",
    "    train_dataset = encoded_dataset[\"train\"],\n",
    "    eval_dataset = encoded_dataset[\"validation\"],\n",
    "    tokenizer = tokenizer,\n",
    "    data_collator = default_data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/zouyuheng/.conda/envs/zyh_pytorch/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 2490\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 156\n",
      "  Number of trainable parameters = 108311810\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='156' max='156' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [156/156 03:30, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.665392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.659249</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 277\n",
      "  Batch size = 32\n",
      "The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 277\n",
      "  Batch size = 32\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=156, training_loss=0.6416334005502554, metrics={'train_runtime': 212.9792, 'train_samples_per_second': 23.383, 'train_steps_per_second': 0.732, 'total_flos': 1310293055692800.0, 'train_loss': 0.6416334005502554, 'epoch': 2.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å‘½åå®ä½“è¯†åˆ«ä»»åŠ¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_dataset, load_metric\n",
    "from transformers import BertTokenizerFast, BertForTokenClassification, TrainingArguments, Trainer, DataCollatorForTokenClassification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset conll2003/conll2003 to /home/zouyuheng/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 983k/983k [00:01<00:00, 959kB/s]  \n",
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset conll2003 downloaded and prepared to /home/zouyuheng/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 482.29it/s]\n",
      "loading file vocab.txt from cache at /home/zouyuheng/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/vocab.txt\n",
      "loading file tokenizer.json from cache at /home/zouyuheng/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/zouyuheng/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/tokenizer_config.json\n",
      "loading configuration file config.json from cache at /home/zouyuheng/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('conll2003')\n",
    "# æ­¤å¤„å¦‚æœå‘ç°æ•°æ®ä¸‹è½½ä¸æˆåŠŸï¼Œå¯ä»¥å¾…ä¼šå„¿å†è¯•\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    # å°†è®­ç»ƒé›†è½¬åŒ–ä¸ºç‰¹å¾å½¢å¼ï¼Œå³åˆ†è¯ä»¥åŠå¯¹é½æ ‡ç­¾\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words = True)\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        # æ­¤å¤„åº”è¯¥æ˜¯æ ‡ç­¾åºåˆ—\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index = i)\n",
    "        # æ‰¾åˆ°å¯¹åº”è¯çš„idï¼Œç”±äºNERä»»åŠ¡ä¸­ï¼ŒBERTä¼šå°†tokenæ‹†åˆ†ï¼ŒåŒä¸€tokenæ‹†åˆ†çš„å­è¯å…±äº«åŒä¸€æ ‡ç­¾ï¼Œå› æ­¤è¯¥æ ‡ç­¾å¯¹åº”äº†ä¸€ä¸ªå­è¯åºåˆ—\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            # éå†æ¯ä¸€ä¸ªå­è¯\n",
    "            if word_idx is None:\n",
    "                 label_ids.append(-100)\n",
    "            # å¦‚æœè¯ä¸ºç©ºï¼Œè¯´æ˜æ˜¯ç‰¹æ®Šç¬¦å·ï¼Œå°†å…¶æ ‡ç­¾è®¾ç½®ä¸º-100ï¼Œä¹‹ååœ¨è®¡ç®—æŸå¤±å‡½æ•°ä¸­å°†å¿½ç•¥\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(label[word_idx])\n",
    "            previous_word_idx = word_idx\n",
    "            # å°†æ ‡ç­¾è®¾ç½®åˆ°æ¯ä¸ªè¯çš„ç¬¬ä¸€ä¸ªtokenä¸Šï¼Œæ­¤å¤„å°±æ˜¯å®ç°å­è¯å…±äº«æ ‡ç­¾çš„æ“ä½œ\n",
    "            '''æ²¡æœ‰ç‰¹åˆ«ç†è§£ä¸ºä»€ä¹ˆè¦åˆ¤æ–­ä¸€ä¸ªelif'''\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    # è®¾ç½®ä¸ºæ ‡ç­¾\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 14/15 [00:01<00:00,  7.14ba/s]\n",
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:00<00:00,  7.08ba/s]\n",
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:00<00:00,  7.48ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'æ­¤å¤„ä¸ºä»€ä¹ˆè¦å°†ä»ç¼“å­˜ä¸­åŠ è½½è®¾ç½®ä¸ºFalse'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_datasets = dataset.map(tokenize_and_align_labels, batched=True, load_from_cache_file=False)\n",
    "'''æ­¤å¤„ä¸ºä»€ä¹ˆè¦å°†ä»ç¼“å­˜ä¸­åŠ è½½è®¾ç½®ä¸ºFalse'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/zouyuheng/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/config.json\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/zouyuheng/.cache/huggingface/hub/models--bert-base-cased/snapshots/5532cc56f74641d4bb33641f5c76a55d11f846e0/pytorch_model.bin\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "label_list = dataset[\"train\"].features[\"ner_tags\"].feature.names\n",
    "# è·å–æ‰€æœ‰çš„æ ‡ç­¾åˆ—è¡¨\n",
    "model = BertForTokenClassification.from_pretrained(\"bert-base-cased\", num_labels = len(label_list))\n",
    "# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "metric = load_metric(\"/home/zouyuheng/tool/huggingface-datasets/seqeval.py\")\n",
    "# æ³¨æ„ï¼Œæ­¤å¤„ä¸å…‰éœ€è¦ä¸‹è½½æ–‡ä»¶åˆ°æœ¬åœ°ï¼Œè€Œä¸”éœ€è¦pipå®‰è£…seqevalåº“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    # å®šä¹‰è¯„ä»·æŒ‡æ ‡\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis = 2)\n",
    "    # å–æ¦‚ç‡æœ€å¤§çš„ä½œä¸ºé¢„æµ‹ç»“æœ\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "        # å–é¢„æµ‹ç»“æœï¼Œå»æ‰æˆ‘ä»¬æ ‡æ³¨ä¸º-100çš„ç‰¹æ®Šç¬¦å·\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "        # å–é¢„æµ‹ç»“æœï¼Œå»æ‰æˆ‘ä»¬æ ‡æ³¨ä¸º-100çš„ç‰¹æ®Šç¬¦å·\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions = true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\" : results[\"overall_precision\"],# ç²¾ç¡®ç‡\n",
    "        \"recall\" : results[\"overall_recall\"], # å¬å›ç‡\n",
    "        \"f1\" : results[\"overall_f1\"], # f1å€¼\n",
    "        \"accuracy\" : results[\"overall_accuracy\"] # å‡†ç¡®ç‡\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "# è®¾ç½®è®­ç»ƒè¶…å‚\n",
    "args = TrainingArguments(\n",
    "    \"ft-conll2003\", # è¾“å‡ºè·¯å¾„\n",
    "    evaluation_strategy = \"epoch\",# æ¯è½®ç»“æŸåè¿›è¡Œè¯„ä»·\n",
    "    learning_rate = 2e-5,\n",
    "    per_device_train_batch_size = 32,# è®­ç»ƒæ‰¹æ¬¡å¤§å°\n",
    "    per_device_eval_batch_size = 32,# éªŒè¯æ‰¹æ¬¡å¤§å°\n",
    "    num_train_epochs = 3 # è®­ç»ƒè½®æ¬¡\n",
    ")\n",
    "\n",
    "# è®­ç»ƒ\n",
    "trainer = Trainer(\n",
    "    model, \n",
    "    args, \n",
    "    train_dataset = tokenize_datasets[\"train\"],\n",
    "    eval_dataset = tokenize_datasets[\"validation\"],\n",
    "    tokenizer = tokenizer,\n",
    "    compute_metrics = compute_metrics,\n",
    "    data_collator = data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: chunk_tags, ner_tags, tokens, id, pos_tags. If chunk_tags, ner_tags, tokens, id, pos_tags are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "/home/zouyuheng/.conda/envs/zyh_pytorch/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 14041\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1317\n",
      "  Number of trainable parameters = 107726601\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='26' max='1317' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  26/1317 00:03 < 03:19, 6.46 it/s, Epoch 0.06/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[1;32m      2\u001b[0m trainer\u001b[39m.\u001b[39mevaluate()\n",
      "File \u001b[0;32m~/.conda/envs/zyh_pytorch/lib/python3.10/site-packages/transformers/trainer.py:1501\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[1;32m   1498\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1499\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1500\u001b[0m )\n\u001b[0;32m-> 1501\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1502\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1503\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1504\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1505\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1506\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/zyh_pytorch/lib/python3.10/site-packages/transformers/trainer.py:1816\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1814\u001b[0m     optimizer_was_run \u001b[39m=\u001b[39m scale_before \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m scale_after\n\u001b[1;32m   1815\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1816\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m   1818\u001b[0m \u001b[39mif\u001b[39;00m optimizer_was_run \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdeepspeed:\n\u001b[1;32m   1819\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlr_scheduler\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/.conda/envs/zyh_pytorch/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:65\u001b[0m, in \u001b[0;36m_LRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m instance\u001b[39m.\u001b[39m_step_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     64\u001b[0m wrapped \u001b[39m=\u001b[39m func\u001b[39m.\u001b[39m\u001b[39m__get__\u001b[39m(instance, \u001b[39mcls\u001b[39m)\n\u001b[0;32m---> 65\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/zyh_pytorch/lib/python3.10/site-packages/torch/optim/optimizer.py:113\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m    112\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 113\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/zyh_pytorch/lib/python3.10/site-packages/transformers/optimization.py:360\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    356\u001b[0m state[\u001b[39m\"\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    358\u001b[0m \u001b[39m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m    359\u001b[0m \u001b[39m# In-place operations to update the averages at the same time\u001b[39;00m\n\u001b[0;32m--> 360\u001b[0m exp_avg\u001b[39m.\u001b[39;49mmul_(beta1)\u001b[39m.\u001b[39;49madd_(grad, alpha\u001b[39m=\u001b[39;49m(\u001b[39m1.0\u001b[39;49m \u001b[39m-\u001b[39;49m beta1))\n\u001b[1;32m    361\u001b[0m exp_avg_sq\u001b[39m.\u001b[39mmul_(beta2)\u001b[39m.\u001b[39maddcmul_(grad, grad, value\u001b[39m=\u001b[39m\u001b[39m1.0\u001b[39m \u001b[39m-\u001b[39m beta2)\n\u001b[1;32m    362\u001b[0m denom \u001b[39m=\u001b[39m exp_avg_sq\u001b[39m.\u001b[39msqrt()\u001b[39m.\u001b[39madd_(group[\u001b[39m\"\u001b[39m\u001b[39meps\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "trainer.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zyh_pytorch",
   "language": "python",
   "name": "zyh_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
