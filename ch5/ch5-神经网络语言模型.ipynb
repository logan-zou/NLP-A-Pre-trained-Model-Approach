{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS_TOKEN = \"<bos>\"\n",
    "EOS_TOKEN = \"<eos>\"\n",
    "PAD_TOKEN = \"<pad>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建Vocab类\n",
    "from collections import defaultdict\n",
    "\n",
    "class Vocab:\n",
    "\n",
    "    def __init__(self, tokens = None) -> None:\n",
    "        self.idx_to_token = list()\n",
    "        self.token_to_idx = dict()\n",
    "\n",
    "        if tokens is not None:\n",
    "            if \"<unk>\" not in tokens:\n",
    "                tokens += [\"<unk>\"]\n",
    "            for token in tokens:\n",
    "                self.idx_to_token.append(token)\n",
    "                self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
    "            self.unk = self.token_to_idx[\"<unk>\"] \n",
    "\n",
    "    @classmethod\n",
    "    def build(cls, text, min_freq = 1, reserved_tokens = None):\n",
    "        # cls 为类本身，相当于Vocab()\n",
    "        token_freqs = defaultdict(int) # 统计token的频率\n",
    "        for sentence in text:\n",
    "            for token in sentence:\n",
    "                token_freqs[token] += 1\n",
    "        uniq_tokens = [\"<unk>\"] + (reserved_tokens if reserved_tokens else [])\n",
    "        uniq_tokens += [token for token, freq in token_freqs.items()  \n",
    "                       if freq >= min_freq and token != \"<unk>\"]\n",
    "        return cls(uniq_tokens)\n",
    "        \n",
    "    def __len__(self):\n",
    "        # 返回词表的大小\n",
    "        return len(self.idx_to_token)\n",
    "\n",
    "    def __getitem__(self, token):\n",
    "        # 查找输入token对应的索引值，如果不存在返回<unk>对应的索引0\n",
    "        return self.token_to_idx.get(token, self.unk)\n",
    "\n",
    "    def convert_tokens_do_ids(self, tokens):\n",
    "        return [self[token] for token in tokens]\n",
    "\n",
    "    def convert_ids_to_tokens(self, indices):\n",
    "        return [self.idx_to_token[index] for index in indices]\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download(\"reuters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import reuters\n",
    "text = reuters.sents()\n",
    "text = [[word.lower() for word in sentence] for sentence in text]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_reuters():\n",
    "    # 从nltk中导入reuters数据\n",
    "    from nltk.corpus import reuters\n",
    "    # 获取reutuers数据\n",
    "    text = reuters.sents()\n",
    "    # 将字母都转化为小写\n",
    "    text = [[word.lower() for word in sentence] for sentence in text]\n",
    "    # 构建词表\n",
    "    vocab = Vocab.build(text, reserved_tokens=[PAD_TOKEN, BOS_TOKEN, EOS_TOKEN])\n",
    "    # 将文本数据转换为id表示\n",
    "    corpus = [vocab.convert_tokens_do_ids(sentence) for sentence in text]\n",
    "    return vocab, corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, corpus = load_reuters()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基于前馈神经网络生成词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个对应数据集\n",
    "# 从torch.utils.Dataset继承\n",
    "class NGramDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, corpus, vocab, context_size = 2):\n",
    "\n",
    "        self.data = []\n",
    "        self.bos = vocab[BOS_TOKEN]# 句首标记\n",
    "        self.eos = vocab[EOS_TOKEN]# 句尾标记\n",
    "\n",
    "        for sentence in tqdm(corpus, desc = \"Dataset Construction\"):\n",
    "            sentence = [self.bos] + sentence + [self.eos] # 插入句首句尾标记符\n",
    "            if len(sentence) < context_size:\n",
    "                continue\n",
    "            for i in range(context_size, len(sentence)):\n",
    "                # 模型输入：长度为context_size的上下文\n",
    "                context = sentence[i-context_size:i]\n",
    "                # 模型输出：当前词\n",
    "                target = sentence[i]\n",
    "                # 每个训练样本由(context, target)组成\n",
    "                self.data.append((context, target))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.data[i]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(examples):\n",
    "    # 从独立样本集合中构建批次的输入输出，并转换为PyTorch张量\n",
    "    inputs = torch.tensor([ex[0] for ex in examples], dtype=torch.long)\n",
    "    targets = torch.tensor([ex[1] for ex in examples], dtype=torch.long)\n",
    "    return (inputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwordNNLM(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, context_size, hidden_dim):\n",
    "        super(FeedForwordNNLM, self).__init__()\n",
    "        # 词向量层，每一个输入都是一个词表维度的one-hot向量；输入是每个单词的索引\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # 线性变换：词向量层——隐含层\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, hidden_dim)\n",
    "        # 线性变换：隐含层——输出层, 输出维度将通过softmax归一化取为词表概率\n",
    "        self.linear2 = nn.Linear(hidden_dim, vocab_size)\n",
    "        # 使用Relu激活函数：小于0的输出置为0\n",
    "        self.activate = nn.ReLU()\n",
    "        self.output = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # 将输入词序列映射为词向量，通过view函数对映射后的词向量序列组成的三维张量\n",
    "        # 进行重构，以完成词向量的拼接\n",
    "        # print(\"输入向量的维度为\", inputs.shape)\n",
    "        # print(\"词向量层输出向量的维度为\", self.embeddings(inputs).shape)\n",
    "        embeds = self.embeddings(inputs).view((inputs.shape[0], -1))\n",
    "        # print(\"使用view拼接之后的向量维度为\", embeds.shape)\n",
    "        hidden = self.activate(self.linear1(embeds))\n",
    "        output = self.linear2(hidden)\n",
    "        # 根据输出层计算概率分布并取对数，计算对数似然\n",
    "        log_probs = self.output(output)\n",
    "        return log_probs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 128 # 词向量维度\n",
    "hidden_dim = 256 # 隐藏层维度\n",
    "batch_size = 64 # 批次大小\n",
    "context_size = 3 # 上下文长度\n",
    "num_epoch = 10 # 迭代次数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dataset Construction: 100%|██████████| 54711/54711 [00:03<00:00, 17347.15it/s]\n"
     ]
    }
   ],
   "source": [
    "# 设置超参\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 读取文本数据，构建训练数据集\n",
    "vocab, corpus = load_reuters()\n",
    "dataset = NGramDataset(corpus, vocab, context_size)\n",
    "data_loader = DataLoader(dataset, batch_size, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "nll_loss = nn.NLLLoss()\n",
    "model = FeedForwordNNLM(len(vocab), embedding_dim, context_size, hidden_dim)\n",
    "device = torch.device(\"cuda\")# if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device=device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainging Epoch 0: 100%|██████████| 163/163 [00:09<00:00, 17.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:1018.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainging Epoch 1: 100%|██████████| 163/163 [00:08<00:00, 18.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:821.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainging Epoch 2: 100%|██████████| 163/163 [00:08<00:00, 18.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:760.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainging Epoch 3: 100%|██████████| 163/163 [00:08<00:00, 18.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:719.21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainging Epoch 4: 100%|██████████| 163/163 [00:08<00:00, 18.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:686.53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainging Epoch 5: 100%|██████████| 163/163 [00:08<00:00, 18.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:659.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainging Epoch 6: 100%|██████████| 163/163 [00:08<00:00, 18.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:636.52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainging Epoch 7: 100%|██████████| 163/163 [00:08<00:00, 18.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:617.73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainging Epoch 8: 100%|██████████| 163/163 [00:08<00:00, 18.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:602.54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainging Epoch 9: 100%|██████████| 163/163 [00:08<00:00, 19.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:590.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "total_losses = []\n",
    "for epoch in range(num_epoch):\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(data_loader, desc = f\"Trainging Epoch {epoch}\"):\n",
    "        # for x in batch:\n",
    "        #     for y in x:\n",
    "        #         print(y)\n",
    "        # inputs = torch.tensor(batch[0]).to(device)\n",
    "        # targets = batch[1]\n",
    "        inputs, targets = [x.to(device) for x in batch]\n",
    "        optimizer.zero_grad()\n",
    "        log_probs = model(inputs)\n",
    "        loss = nll_loss(log_probs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Loss:{total_loss:.2f}\")\n",
    "    total_losses.append(total_loss)\n",
    "\n",
    "# save_pretrained(vocab, model.embeddings.weight.data, \"ffnnlm.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pretrained(vocab, embeds, save_path):\n",
    "    with open(save_path, \"w\") as writer:\n",
    "        writer.write(f\"{embeds.shape[0]} {embeds.shape[1]}\\n\")\n",
    "        for idx, token in enumerate(vocab.idx_to_token):\n",
    "            vec = \" \".join([f\"{x}\" for x in embeds[idx]])\n",
    "            writer.write(f\"{token} {vec}\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基于循环神经网络生成词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RnnlmDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, vocab, corpus):\n",
    "        self.data = []\n",
    "        self.bos = vocab[BOS_TOKEN]\n",
    "        self.eos = vocab[EOS_TOKEN]\n",
    "        self.pad = vocab[PAD_TOKEN]\n",
    "        for sentence in tqdm(corpus, desc = \"Dataset Construction\"):\n",
    "            # 输入序列：BOS_TOKEN，w1，w2......\n",
    "            input = [self.bos] + sentence\n",
    "            # 输出序列：w1，w2，EOS_TOKEN\n",
    "            target = sentence + [self.eos]\n",
    "            self.data.append((input, target))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(examples, pad):\n",
    "    # 从独立样本集合中构建批次的输入输出，并转换为PyTorch张量\n",
    "    # for i in range(len(examples)):\n",
    "    #     if len(examples[i][0]) != 50:\n",
    "    #         print(i)\n",
    "    inputs = [torch.tensor(ex[0]) for ex in examples]\n",
    "    targets = [torch.tensor(ex[1]) for ex in examples]\n",
    "    # 注意此处先生成了列表，而不是如同前文一样生成tensor，因为tensor需要补齐\n",
    "    inputs = pad_sequence(inputs, batch_first=True, padding_value=pad)\n",
    "    targets = pad_sequence(targets, batch_first=True, padding_value=pad)\n",
    "    return (inputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNLM(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(RNNLM, self).__init__()\n",
    "        # 词向量层\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # LSTM\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, batch_first = True)\n",
    "        # 输出层\n",
    "        self.output = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=2)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embedding(inputs)\n",
    "        hidden, _ = self.rnn(embeds)\n",
    "        output = self.output(hidden)\n",
    "        log_probs = self.softmax(output)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dataset Construction: 100%|██████████| 54711/54711 [00:00<00:00, 199319.17it/s]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "vocab, corpus = load_reuters()\n",
    "dataset = RnnlmDataset(vocab, corpus)\n",
    "data_loader = DataLoader(dataset, batch_size, collate_fn=lambda x : collate_fn(x, vocab[PAD_TOKEN]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNLM(\n",
       "  (embedding): Embedding(31081, 128)\n",
       "  (rnn): LSTM(128, 256, batch_first=True)\n",
       "  (output): Linear(in_features=256, out_features=31081, bias=True)\n",
       "  (softmax): LogSoftmax(dim=2)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 设置ignore_index参数，以忽略PAD_TOKEN处的损失\n",
    "nll_loss = nn.NLLLoss(ignore_index=dataset.pad)\n",
    "model = RNNLM(len(vocab), embedding_dim, hidden_dim)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 0: 100%|██████████| 855/855 [00:32<00:00, 26.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:4960.68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 855/855 [00:32<00:00, 26.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:4112.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 855/855 [00:32<00:00, 25.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:3795.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 855/855 [00:31<00:00, 27.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:3585.39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 855/855 [00:32<00:00, 26.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:3427.35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 855/855 [00:33<00:00, 25.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:3297.07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6: 100%|██████████| 855/855 [00:33<00:00, 25.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:3185.90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7: 100%|██████████| 855/855 [00:33<00:00, 25.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:3090.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8: 100%|██████████| 855/855 [00:32<00:00, 26.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:3005.44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9: 100%|██████████| 855/855 [00:32<00:00, 26.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:2929.91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epoch):\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(data_loader, desc = f\"Training Epoch {epoch}\"):\n",
    "        inputs, targets = [x.to(device) for x in batch]\n",
    "        optimizer.zero_grad()\n",
    "        log_probs = model(inputs)\n",
    "        loss = nll_loss(log_probs.view(-1, log_probs.shape[-1]), targets.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Loss:{total_loss:.2f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec词向量"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CbowDataset(Dataset):\n",
    "    # 用于CBOW模型的数据集\n",
    "    def __init__(self, vocab, corpus, context_size = 2) -> None:\n",
    "        self.data = []\n",
    "        self.bos = vocab[BOS_TOKEN]\n",
    "        self.eos = vocab[EOS_TOKEN]\n",
    "        for sentence in tqdm(corpus, desc=\"Dataset Construction\"):\n",
    "            sentence = [self.bos] + sentence + [self.eos]\n",
    "            if len(sentence) < context_size * 2 + 1:\n",
    "                # 此处的context_size是单向上下文长度，因此，如果小于上文长度，无法构建该任务\n",
    "                continue\n",
    "            for i in range(context_size, len(sentence) - context_size):\n",
    "                context = sentence[i-context_size:i] + sentence[i+1:i+context_size]\n",
    "                # 模型输入：左右各取context_size的上下文\n",
    "                target = sentence[i]\n",
    "                # 模型输出：中间的单词\n",
    "                self.data.append((context, target))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CBOW模型\n",
    "class CbowModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim) -> None:\n",
    "        super(CbowModel, self).__init__()\n",
    "\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # 词向量层\n",
    "        self.output = nn.Linear(embedding_dim, vocab_size, bias=False)\n",
    "        # 输出层\n",
    "        self.log = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs)\n",
    "        hidden = embeds.mean(dim=1)\n",
    "        # 对词向量取平均\n",
    "        output = self.output(hidden)\n",
    "        log_probs = self.log(output)\n",
    "        return log_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(examples):\n",
    "    # 从独立样本集合中构建批次的输入输出，并转换为PyTorch张量\n",
    "    inputs = torch.tensor([ex[0] for ex in examples], dtype=torch.long)\n",
    "    targets = torch.tensor([ex[1] for ex in examples], dtype=torch.long)\n",
    "    return (inputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dataset Construction: 100%|██████████| 54711/54711 [00:03<00:00, 16829.04it/s]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "vocab, corpus = load_reuters()\n",
    "dataset = CbowDataset(vocab, corpus)\n",
    "data_loader = DataLoader(dataset, batch_size, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置ignore_index参数，以忽略PAD_TOKEN处的损失\n",
    "nll_loss = nn.NLLLoss()\n",
    "model = CbowModel(len(vocab), embedding_dim)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 128 # 词向量维度\n",
    "hidden_dim = 256 # 隐藏层维度\n",
    "batch_size = 10240 # 批次大小\n",
    "context_size = 3 # 上下文长度\n",
    "num_epoch = 10 # 迭代次数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainging Epoch 0: 100%|██████████| 158/158 [00:05<00:00, 28.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:1399.43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainging Epoch 1: 100%|██████████| 158/158 [00:05<00:00, 29.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:1002.09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainging Epoch 2: 100%|██████████| 158/158 [00:05<00:00, 27.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:891.18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainging Epoch 3: 100%|██████████| 158/158 [00:05<00:00, 27.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:835.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainging Epoch 4: 100%|██████████| 158/158 [00:05<00:00, 27.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:797.48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainging Epoch 5: 100%|██████████| 158/158 [00:05<00:00, 27.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:768.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainging Epoch 6: 100%|██████████| 158/158 [00:05<00:00, 27.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:746.11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainging Epoch 7: 100%|██████████| 158/158 [00:05<00:00, 27.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:727.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainging Epoch 8: 100%|██████████| 158/158 [00:05<00:00, 27.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:710.85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainging Epoch 9: 100%|██████████| 158/158 [00:05<00:00, 27.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:696.69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "total_losses = []\n",
    "for epoch in range(num_epoch):\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(data_loader, desc = f\"Trainging Epoch {epoch}\"):\n",
    "        # for x in batch:\n",
    "        #     for y in x:\n",
    "        #         print(y)\n",
    "        # inputs = torch.tensor(batch[0]).to(device)\n",
    "        # targets = batch[1]\n",
    "        inputs, targets = [x.to(device) for x in batch]\n",
    "        optimizer.zero_grad()\n",
    "        log_probs = model(inputs)\n",
    "        loss = nll_loss(log_probs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Loss:{total_loss:.2f}\")\n",
    "    total_losses.append(total_loss)\n",
    "\n",
    "# save_pretrained(vocab, model.embeddings.weight.data, \"ffnnlm.vec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skip-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramDataset(Dataset):\n",
    "    # 定义用于Skip-gram模型的数据集\n",
    "    def __init__(self, vocab, corpus, context_size=2) -> None:\n",
    "        self.data = []\n",
    "        self.bos = vocab[BOS_TOKEN]\n",
    "        self.eos = vocab[EOS_TOKEN]\n",
    "        for sentence in tqdm(corpus, desc=\"Dataset Construction\"):\n",
    "            sentence = [self.bos] + sentence + [self.eos]\n",
    "            for i in range(1, len(sentence) - 1):\n",
    "                # 从第二个单词开始，到倒数第二个词为之\n",
    "                w = sentence[i]\n",
    "                # 模型输入：当前词\n",
    "                left_context_index = max(0, i - context_size)\n",
    "                right_context_index = min(len(sentence), i + context_size)\n",
    "                context = sentence[left_context_index:i] + sentence[i+1:right_context_index]\n",
    "                # 模型输出：上下文窗口内的共现词，如果窗口边缘超出了字符串左右，则截取到字符串尽头\n",
    "                self.data.extend([(w, c) for c in context])\n",
    "                # 此处使用extend，因为该列表里面是多个词对，每一个是一个输出\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramModule(nn.Module):\n",
    "    # 定义一个Skip-gram的模型\n",
    "    def __init__(self, vocab_size, embedding_dim) -> None:\n",
    "        super(SkipGramModule, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # 词向量层\n",
    "        self.output = nn.Linear(embedding_dim, vocab_size, bias=False)\n",
    "        # 线性层\n",
    "        self.log = nn.LogSoftmax(dim=1)\n",
    "        # 分类层\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embedding(inputs)\n",
    "        output = self.output(embeds)\n",
    "        log_probs = self.log(output)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(examples):\n",
    "    # 从独立样本集合中构建批次的输入输出，并转换为PyTorch张量\n",
    "    inputs = torch.tensor([ex[0] for ex in examples], dtype=torch.long)\n",
    "    targets = torch.tensor([ex[1] for ex in examples], dtype=torch.long)\n",
    "    return (inputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 128 # 词向量维度\n",
    "hidden_dim = 256 # 隐藏层维度\n",
    "batch_size = 40960 # 批次大小\n",
    "context_size = 3 # 上下文长度\n",
    "num_epoch = 10 # 迭代次数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dataset Construction: 100%|██████████| 54711/54711 [00:02<00:00, 27211.21it/s]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "vocab, corpus = load_reuters()\n",
    "dataset = SkipGramDataset(vocab, corpus)\n",
    "data_loader = DataLoader(dataset, batch_size, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置ignore_index参数，以忽略PAD_TOKEN处的损失\n",
    "nll_loss = nn.NLLLoss()\n",
    "model = SkipGramModule(len(vocab), embedding_dim)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainging Epoch 0: 100%|██████████| 125/125 [00:15<00:00,  8.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:1168.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainging Epoch 1: 100%|██████████| 125/125 [00:15<00:00,  8.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:931.68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainging Epoch 2: 100%|██████████| 125/125 [00:15<00:00,  8.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:824.61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainging Epoch 3: 100%|██████████| 125/125 [00:15<00:00,  7.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:787.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainging Epoch 4: 100%|██████████| 125/125 [00:15<00:00,  7.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:768.44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainging Epoch 5: 100%|██████████| 125/125 [00:15<00:00,  7.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:756.21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainging Epoch 6: 100%|██████████| 125/125 [00:15<00:00,  7.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:747.16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainging Epoch 7: 100%|██████████| 125/125 [00:15<00:00,  7.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:740.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainging Epoch 8: 100%|██████████| 125/125 [00:15<00:00,  7.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:734.09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainging Epoch 9: 100%|██████████| 125/125 [00:15<00:00,  7.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:729.07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "total_losses = []\n",
    "for epoch in range(num_epoch):\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(data_loader, desc = f\"Trainging Epoch {epoch}\"):\n",
    "        # for x in batch:\n",
    "        #     for y in x:\n",
    "        #         print(y)\n",
    "        # inputs = torch.tensor(batch[0]).to(device)\n",
    "        # targets = batch[1]\n",
    "        inputs, targets = [x.to(device) for x in batch]\n",
    "        optimizer.zero_grad()\n",
    "        log_probs = model(inputs)\n",
    "        loss = nll_loss(log_probs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Loss:{total_loss:.2f}\")\n",
    "    total_losses.append(total_loss)\n",
    "\n",
    "# save_pretrained(vocab, model.embeddings.weight.data, \"ffnnlm.vec\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用负采样技术的Skip-Gram\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "负采样理论介绍：https://zhuanlan.zhihu.com/p/39684349"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGNSDataset(Dataset):\n",
    "# 用于负采样的Skip-Gram的数据集\n",
    "# 我们在数据集构建时生成负样本\n",
    "    def __init__(self, vocab, corpus, context_size=2, n_negatives=5, ns_dist = None) -> None:\n",
    "        # n_negative指生成负样本个数\n",
    "        # ns_dist指生成负样本分布，None为均匀分布\n",
    "        self.data = []\n",
    "        self.bos = vocab[BOS_TOKEN]\n",
    "        self.eos = vocab[EOS_TOKEN]\n",
    "        self.pad = vocab[PAD_TOKEN]\n",
    "        for sentence in tqdm(corpus, desc = \"Dataset Construction\"):\n",
    "            sentence = [self.bos] + sentence + [self.eos]\n",
    "            for i in range(1, len(sentence) - 1):\n",
    "                w = sentence[i]\n",
    "                left_context_index = max(0, i - context_size)\n",
    "                right_context_index = min(len(sentence), i + context_size)\n",
    "                context = sentence[left_context_index:i] + sentence[i+1:right_context_index]\n",
    "                # 模型输入为当前词和上下文的词对\n",
    "                context += [self.pad] * (2 * context_size - len(context))\n",
    "                # 需要对上下文进行补齐\n",
    "                self.data.append((w, context))\n",
    "                # 输出为0/1，标志是否为负样本\n",
    "\n",
    "        self.n_negatives = n_negatives\n",
    "        self.ns_dist = ns_dist if ns_dist == None else torch.ones(len(vocab))\n",
    "        # None为均匀分布\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.data[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(examples, ns_dist, n_negatives):\n",
    "    # 从独立样本集合中构建批次的输入输出，并转换为PyTorch张量\n",
    "    words = torch.tensor([ex[0] for ex in examples], dtype=torch.long)\n",
    "    contexts = torch.tensor([ex[1] for ex in examples], dtype=torch.long)\n",
    "    batch_size, context_size = contexts.shape\n",
    "    neg_contexts = []\n",
    "    # 对批次内样本分别进行负采样\n",
    "    for i in range(batch_size):\n",
    "        ns_dist = ns_dist.index_fill(0, contexts[i], .0)\n",
    "        # index_fill根据给定索引填充张量的值，此处用于保持context不变\n",
    "        neg_contexts.append(torch.multinomial(ns_dist, n_negatives * context_size, replacement=True))\n",
    "        # 对input的每一行做n_samples次取值，输出的张量是每一次取值时input张量对应行的下标\n",
    "    neg_contexts = torch.stack(neg_contexts, dim=0)\n",
    "    # stack用于拼接\n",
    "    return words, contexts, neg_contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGNSModel(nn.Module):\n",
    "    # 用于负采样技术的Skip-Gram模型\n",
    "    # 该模型分别维护并训练词和上下文的向量层\n",
    "    def __init__(self, vocab_size, embedding_dim) -> None:\n",
    "        super(SGNSModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # 词向量层\n",
    "        self.context_embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # 上下文词向量层\n",
    "        self.output = nn.Linear(embedding_dim, vocab_size, bias=False)\n",
    "        # 线性层\n",
    "        self.log = nn.LogSoftmax(dim=1)\n",
    "        # 分类层\n",
    "\n",
    "    def forward_w(self, words):\n",
    "        words_embeds = self.embedding(words)\n",
    "        return words_embeds\n",
    "\n",
    "    def forward_c(self, contexts):\n",
    "        contexts_embeds = self.embedding(contexts)\n",
    "        return contexts_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算训练预料中Unigram概率分布\n",
    "def get_unigram_distribution(corpus, vocab_size):\n",
    "    token_counts = torch.tensor([0] * vocab_size)\n",
    "    total_count = 0\n",
    "    for sentence in corpus:\n",
    "        total_count += len(sentence)\n",
    "        for token in sentence:\n",
    "            token_counts[token] += 1\n",
    "    unigram_dist = torch.div(token_counts.float(), total_count)\n",
    "    # 做除法，即将频次转化为频率\n",
    "    return unigram_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 128 # 词向量维度\n",
    "batch_size = 10240 # 批次大小\n",
    "context_size = 3 # 上下文长度\n",
    "n_negatives = 5 # 负样本数量\n",
    "num_epoch = 10 # 迭代次数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dataset Construction: 100%|██████████| 54711/54711 [00:06<00:00, 8933.66it/s] \n"
     ]
    }
   ],
   "source": [
    "vocab, corpus = load_reuters()\n",
    "unigram_dist = get_unigram_distribution(corpus, len(vocab))\n",
    "# 计算unigram概率分布\n",
    "negative_sampling_dist = unigram_dist ** 0.75\n",
    "negative_sampling_dist /= negative_sampling_dist.sum()\n",
    "# 计算负采样分布\n",
    "dataset = SGNSDataset(vocab, corpus, context_size, n_negatives, negative_sampling_dist)\n",
    "data_loader = DataLoader(dataset, batch_size, collate_fn=lambda x : collate_fn(x, negative_sampling_dist, n_negatives))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = SGNSModel(len(vocab), embedding_dim)\n",
    "model.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 0: 100%|██████████| 169/169 [02:27<00:00,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:4128.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 169/169 [02:13<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:3404.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 169/169 [02:13<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:2866.49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 169/169 [02:12<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:2452.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 169/169 [02:11<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:2134.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 169/169 [02:12<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:1886.13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6: 100%|██████████| 169/169 [02:12<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:1688.63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7: 100%|██████████| 169/169 [02:13<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:1526.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8: 100%|██████████| 169/169 [02:15<00:00,  1.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:1390.79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9: 100%|██████████| 169/169 [02:12<00:00,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:1275.35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(num_epoch):\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(data_loader, desc=f\"Training Epoch {epoch}\"):\n",
    "        words, contexts, neg_contexts = [x.to(device) for x in batch]\n",
    "        optimizer.zero_grad()\n",
    "        batch_size = words.shape[0]\n",
    "        words_embeds = model.forward_w(words).unsqueeze(dim=2)\n",
    "        contexts_embeds = model.forward_c(contexts)\n",
    "        neg_contexts_embeds = model.forward_c(neg_contexts)\n",
    "        # 分别提取其向量表示\n",
    "        context_loss = torch.nn.functional.logsigmoid(torch.bmm(contexts_embeds, words_embeds).squeeze(dim=2))\n",
    "        context_loss = context_loss.mean(dim=1)\n",
    "        # bmm函数：矩阵乘法\n",
    "        # 正样本的对数似然\n",
    "        neg_context_loss = torch.nn.functional.logsigmoid(torch.bmm(neg_contexts_embeds, words_embeds).squeeze(dim=2).neg())\n",
    "        neg_context_loss = neg_context_loss.view(batch_size, -1, n_negatives).sum(dim=2)\n",
    "        neg_context_loss = neg_context_loss.mean(dim=1)\n",
    "        # neg函数：返回负数\n",
    "        # 负样本的对数似然\n",
    "        loss = -(context_loss + neg_context_loss).mean()\n",
    "        # 总损失\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Loss:{total_loss:.2f}\")\n",
    "\n",
    "combined_embeds = model.embedding.weight + model.context_embedding.weight\n",
    "# 拼接两个词向量得到最终词向量"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zyh_pytorch",
   "language": "python",
   "name": "zyh_pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b726feef97ab7aa43c9328b17888a42dfc64f3ed1b74ed90467738a64aa6d35f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
