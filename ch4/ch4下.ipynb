{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实现《自然语言处理——预训练模型方法》第四章代码下（4.6~4.7）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 情感分类词表准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个词表类型\n",
    "# 该类用于实现token到索引的映射\n",
    "class Vocab:\n",
    "\n",
    "    def __init__(self, tokens = None) -> None:\n",
    "        # 构造函数\n",
    "        # tokens：全部的token列表\n",
    "\n",
    "        self.idx_to_token = list()\n",
    "        # 将token存成列表，索引直接查找对应的token即可\n",
    "        self.token_to_idx = dict()\n",
    "        # 将索引到token的映射关系存成字典，键为索引，值为对应的token\n",
    "\n",
    "        if tokens is not None:\n",
    "            # 构造时输入了token的列表\n",
    "            if \"<unk>\" not in tokens:\n",
    "                # 不存在标记\n",
    "                tokens = tokens + \"<unk>\"\n",
    "            for token in tokens:\n",
    "                self.idx_to_token.append(token)\n",
    "                self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
    "                # 当前该token对应的索引是当下列表的最后一个\n",
    "            self.unk = self.token_to_idx[\"<unk>\"]\n",
    "\n",
    "    @classmethod\n",
    "    def build(cls, text, min_freq=1, reserved_tokens=None):\n",
    "        # 构建词表\n",
    "        # cls：该类本身\n",
    "        # text：输入的文本\n",
    "        # min_freq：列入token的最小频率\n",
    "        # reserved_tokens：额外的标记token\n",
    "        token_freqs = defaultdict(int)\n",
    "        for sentence in text:\n",
    "            for token in sentence:\n",
    "                token_freqs[token] += 1\n",
    "        # 统计各个token的频率\n",
    "        uniq_tokens = [\"<unk>\"] + (reserved_tokens if reserved_tokens else [])\n",
    "        # 加入额外的token\n",
    "        uniq_tokens += [token for token, freq in token_freqs.items() \\\n",
    "            if freq >= min_freq and token != \"<unk>\"]\n",
    "        # 全部的token列表\n",
    "        return cls(uniq_tokens)\n",
    "\n",
    "    def __len__(self):\n",
    "        # 返回词表的大小\n",
    "        return len(self.idx_to_token)\n",
    "\n",
    "    def __getitem__(self, token):\n",
    "        # 查找输入token对应的索引，不存在则返回<unk>返回的索引\n",
    "        return self.token_to_idx.get(token, self.unk)\n",
    "\n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        # 查找一系列输入标签对应的索引值\n",
    "        return [self[token] for token in tokens]\n",
    "\n",
    "    def convert_ids_to_tokens(self, ids):\n",
    "        # 查找一系列索引值对应的标记\n",
    "        return [self.idx_to_token[index] for index in ids]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 多层感知机"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\DemoEditor\\Anaconda\\envs\\env_for_gpu\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个MLP类\n",
    "class MLP(nn.Module):\n",
    "    # 基类为nn.Module\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_class):\n",
    "        # 构造函数\n",
    "        # vocab_size:词表大小\n",
    "        # embedding_dim：词向量维度\n",
    "        # hidden_dim:隐藏层维度\n",
    "        # num_class:多分类个数\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embedding_dim)\n",
    "        # 词向量层\n",
    "        self.linear1 = nn.Linear(embedding_dim, hidden_dim)\n",
    "        # 隐含层，线性变换\n",
    "        self.activate = F.relu\n",
    "        # 使用relu函数作为激活函数：小于0的值输出为0\n",
    "        self.linear2 = nn.Linear(hidden_dim, num_class)\n",
    "        # 输出层，线性变换\n",
    "\n",
    "    def forward(self, inputs, offsets):\n",
    "        # 前向计算函数\n",
    "        # inputs:输入\n",
    "        # print(f\"输入为：{inputs.size()}\")\n",
    "        embeds = self.embedding(inputs, offsets)\n",
    "        # 对词向量层取袋模型\n",
    "        hidden = self.linear1(embeds)\n",
    "        # print(f\"经过隐含层变换为：{hidden}\")\n",
    "        activation = self.activate(hidden)\n",
    "        # print(f\"经过激活后为：{activation}\")\n",
    "        outputs = self.linear2(activation)\n",
    "        # print(f\"输出层输出为：{outputs}\")\n",
    "        probs = F.log_softmax(outputs, dim = 1)\n",
    "        # print(f\"输出概率值为：{probs}\")\n",
    "        # 归一化为概率值\n",
    "        return probs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import sentence_polarity\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sentence_polarity():\n",
    "\n",
    "    vocab = Vocab.build(sentence_polarity.sents())\n",
    "    # 使用nltk的情感倾向数据作为示例\n",
    "\n",
    "    train_data = [(vocab.convert_tokens_to_ids(sentence), 0) for sentence in sentence_polarity.sents(categories=\"pos\")[:4000]]\\\n",
    "        +[(vocab.convert_tokens_to_ids(sentence), 1) for sentence in sentence_polarity.sents(categories='neg')[:4000]]\n",
    "    # 分别取褒贬各4000句作为训练数据，将token映射为对应的索引值\n",
    "\n",
    "    test_data = [(vocab.convert_tokens_to_ids(sentence), 0) for sentence in sentence_polarity.sents(categories=\"pos\")[4000:]]\\\n",
    "        +[(vocab.convert_tokens_to_ids(sentence), 1) for sentence in sentence_polarity.sents(categories=\"neg\")[4000:]]\n",
    "    # 其余数据作为测试数据\n",
    "\n",
    "    return train_data, test_data, vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 声明一个DataSet类\n",
    "class BowDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data) -> None:\n",
    "        # data：使用load_sentence_polarity获得的数据\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        # 返回样例的数目\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collate_fn函数用于对一个批次的样本进行整理\n",
    "def collate_fn(examples):\n",
    "    # 从独立样本集合中构建各批次的输入输出\n",
    "    inputs = [torch.tensor(ex[0]) for ex in examples]\n",
    "    # 将输入inputs定义为一个张量的列表，每一个张量为句子对应的索引值序列\n",
    "    targets = torch.tensor([ex[1] for ex in examples], dtype=torch.long)\n",
    "    # 目标targets为该批次所有样例输出结果构成的张量\n",
    "    offsets = [0] + [i.shape[0] for i in inputs]\n",
    "    # 一个批次中每个样例的序列长度\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    # 根据序列长度计算每个序列起始位置的偏移量\n",
    "    inputs = torch.cat(inputs)\n",
    "    # 将inputs列表中的张量拼接成一个大的张量\n",
    "    return inputs, offsets, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 训练多层感知机模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (embedding): EmbeddingBag(21402, 128, mode=mean)\n",
       "  (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
       "  (linear2): Linear(in_features=256, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 超参数设置\n",
    "embedding_dim = 128\n",
    "hidden_dim = 256\n",
    "num_class = 2\n",
    "batch_size = 16\n",
    "num_epoch = 10\n",
    "\n",
    "train_data, test_data, vocab = load_sentence_polarity()\n",
    "# 加载数据\n",
    "train_dataset = BowDataset(train_data)\n",
    "test_dataset = BowDataset(test_data)\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=True)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=1, collate_fn=collate_fn, shuffle=False)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = MLP(len(vocab), embedding_dim, hidden_dim, num_class)\n",
    "model.to(device)\n",
    "# 加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "nll_loss = nn.NLLLoss()\n",
    "# 负对数似然损失\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# Adam优化器\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 0: 100%|██████████| 500/500 [00:01<00:00, 442.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:345.66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 2662/2662 [00:01<00:00, 1547.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC:0.52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 500/500 [00:00<00:00, 502.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:345.66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 2662/2662 [00:01<00:00, 1506.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC:0.52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 500/500 [00:01<00:00, 466.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:345.66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 2662/2662 [00:01<00:00, 1561.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC:0.52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 500/500 [00:01<00:00, 453.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:345.66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 2662/2662 [00:01<00:00, 1655.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC:0.52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 500/500 [00:01<00:00, 495.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:345.66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 2662/2662 [00:01<00:00, 1755.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC:0.52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 500/500 [00:00<00:00, 522.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:345.66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 2662/2662 [00:01<00:00, 1746.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC:0.52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6: 100%|██████████| 500/500 [00:00<00:00, 500.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:345.66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 2662/2662 [00:01<00:00, 1689.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC:0.52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7: 100%|██████████| 500/500 [00:01<00:00, 496.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:345.66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 2662/2662 [00:01<00:00, 1777.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC:0.52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8: 100%|██████████| 500/500 [00:00<00:00, 551.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:345.66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 2662/2662 [00:01<00:00, 1837.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC:0.52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9: 100%|██████████| 500/500 [00:00<00:00, 525.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:345.66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 2662/2662 [00:01<00:00, 1829.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC:0.52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for epoch in range(num_epoch):\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_data_loader, desc=f\"Training Epoch {epoch}\"):\n",
    "        inputs, offsets, targets = [x.to(device) for x in batch]\n",
    "        log_probs = model(inputs, offsets)\n",
    "        loss = nll_loss(log_probs, targets)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Loss:{total_loss:.2f}\")\n",
    "\n",
    "    # 测试\n",
    "    acc = 0\n",
    "    for batch in tqdm(test_data_loader, desc=f\"Testing\"):\n",
    "        inputs, offsets, targets = [x.to(device) for x in batch]\n",
    "        with torch.no_grad():\n",
    "            output = model(inputs, offsets)\n",
    "            acc += (output.argmax(dim=1) == targets).sum().item()\n",
    "    print(f\"ACC:{acc / len(test_data_loader):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. 训练卷积神经网络模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个CNN类\n",
    "class CNN(nn.Module):\n",
    "    # 基类为nn.Module\n",
    "    def __init__(self, vocab_size, embedding_dim, filter_size, num_filter, num_class):\n",
    "        # 构造函数\n",
    "        # vocab_size:词表大小\n",
    "        # embedding_dim：词向量维度\n",
    "        # filter_size：卷积核大小\n",
    "        # num_filter: 卷积核个数\n",
    "        # num_class:多分类个数\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # 词向量层\n",
    "        self.conv1d = nn.Conv1d(embedding_dim, num_filter, filter_size, padding=1)\n",
    "        # 卷积层，使用1作为padding\n",
    "        self.activate = F.relu\n",
    "        # 使用relu函数作为激活函数：小于0的值输出为0\n",
    "        self.linear = nn.Linear(num_filter, num_class)\n",
    "        # 输出层，线性变换\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # 前向计算函数\n",
    "        # inputs:输入\n",
    "        # print(f\"输入为：{inputs.size()}\")\n",
    "        embeds = self.embedding(inputs).permute(0, 2, 1)\n",
    "        # 注意这儿是词向量层，不是词袋词向量层\n",
    "        # 卷积层的输入两个维度与词向量层输出相反，需要使用permute转换一下\n",
    "        # print(f\"词向量层输出为：{embeds.size()}\")\n",
    "        convolution = self.conv1d(embeds)\n",
    "        # print(f\"经过卷积层变换为：{convolution.size()}\")\n",
    "        activation = self.activate(convolution)\n",
    "        # print(f\"经过激活后为：{activation.size()}\")\n",
    "        pooling = F.max_pool1d(activation, kernel_size=activation.shape[-1])\n",
    "        # print(f\"池化后为：{pooling.size()}\")\n",
    "        # print(f\"池化后结果为：{pooling}\")\n",
    "        outputs = self.linear(pooling.squeeze(dim=2))\n",
    "        # 池化后的输出是二维的，需要使用squeeze降维到一维\n",
    "        # print(f\"输出层输出为：{outputs.size()}\")\n",
    "        log_probs = F.log_softmax(outputs, dim = 1)\n",
    "        # print(f\"输出概率值为：{probs}\")\n",
    "        # 归一化为概率值\n",
    "        return log_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 修改collate_fn函数\n",
    "def collate_fn(examples):\n",
    "    # 从独立样本集合中构建各批次的输入输出\n",
    "    inputs = [torch.tensor(ex[0]) for ex in examples]\n",
    "    # 将输入inputs定义为一个张量的列表，每一个张量为句子对应的索引值序列\n",
    "    targets = torch.tensor([ex[1] for ex in examples], dtype=torch.long)\n",
    "    # 目标targets为该批次所有样例输出结果构成的张量\n",
    "    inputs = pad_sequence(inputs, batch_first=True)\n",
    "    # 将用pad_sequence对批次类的样本进行补齐\n",
    "    return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (embedding): Embedding(21402, 128)\n",
       "  (conv1d): Conv1d(128, 150, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "  (linear): Linear(in_features=150, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 训练\n",
    "# 超参数设置\n",
    "embedding_dim = 128\n",
    "filter_size = 3\n",
    "num_filter = 150\n",
    "num_class = 2\n",
    "batch_size = 16\n",
    "num_epoch = 10\n",
    "\n",
    "train_data, test_data, vocab = load_sentence_polarity()\n",
    "# 加载数据\n",
    "train_dataset = BowDataset(train_data)\n",
    "test_dataset = BowDataset(test_data)\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=True)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=1, collate_fn=collate_fn, shuffle=False)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = CNN(len(vocab), embedding_dim, filter_size, num_filter, num_class)\n",
    "model.to(device)\n",
    "# 加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 0: 100%|██████████| 500/500 [00:04<00:00, 110.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:325.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 2662/2662 [00:01<00:00, 1486.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC:0.66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 500/500 [00:02<00:00, 190.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:226.80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 2662/2662 [00:01<00:00, 1484.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC:0.69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 500/500 [00:02<00:00, 216.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:103.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 2662/2662 [00:01<00:00, 1518.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC:0.69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 500/500 [00:02<00:00, 218.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:28.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 2662/2662 [00:01<00:00, 1491.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC:0.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 500/500 [00:02<00:00, 209.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:6.69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 2662/2662 [00:01<00:00, 1438.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC:0.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 500/500 [00:02<00:00, 214.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:2.45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 2662/2662 [00:01<00:00, 1454.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC:0.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6: 100%|██████████| 500/500 [00:02<00:00, 207.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:1.41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 2662/2662 [00:01<00:00, 1386.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC:0.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7: 100%|██████████| 500/500 [00:02<00:00, 206.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 2662/2662 [00:01<00:00, 1471.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC:0.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8: 100%|██████████| 500/500 [00:02<00:00, 215.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 2662/2662 [00:01<00:00, 1529.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC:0.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9: 100%|██████████| 500/500 [00:02<00:00, 217.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:0.43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 2662/2662 [00:01<00:00, 1402.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC:0.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "nll_loss = nn.NLLLoss()\n",
    "# 负对数似然损失\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# Adam优化器\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epoch):\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_data_loader, desc=f\"Training Epoch {epoch}\"):\n",
    "        inputs, targets = [x.to(device) for x in batch]\n",
    "        # print(inputs.size())\n",
    "        log_probs = model(inputs)\n",
    "        loss = nll_loss(log_probs, targets)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Loss:{total_loss:.2f}\")\n",
    "\n",
    "    # 测试\n",
    "    acc = 0\n",
    "    for batch in tqdm(test_data_loader, desc=f\"Testing\"):\n",
    "        inputs, targets = [x.to(device) for x in batch]\n",
    "        with torch.no_grad():\n",
    "            output = model(inputs)\n",
    "            acc += (output.argmax(dim=1) == targets).sum().item()\n",
    "    print(f\"ACC:{acc / len(test_data_loader):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. 训练循环神经网络模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个LSTM类\n",
    "class LSTM(nn.Module):\n",
    "    # 基类为nn.Module\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_class):\n",
    "        # 构造函数\n",
    "        # vocab_size:词表大小\n",
    "        # embedding_dim：词向量维度\n",
    "        # hidden_dim：隐藏层维度\n",
    "        # num_class:多分类个数\n",
    "        super(LSTM, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # 词向量层\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first = True)\n",
    "        # lstm层\n",
    "        self.output = nn.Linear(hidden_dim, num_class)\n",
    "        # 输出层，线性变换\n",
    "\n",
    "    def forward(self, inputs, lengths):\n",
    "        # 前向计算函数\n",
    "        # inputs:输入\n",
    "        # lengths:打包的序列长度\n",
    "        # print(f\"输入为：{inputs.size()}\")\n",
    "        embeds = self.embedding(inputs)\n",
    "        # 注意这儿是词向量层，不是词袋词向量层\n",
    "        # print(f\"词向量层输出为：{embeds.size()}\")\n",
    "        x_pack = pack_padded_sequence(embeds, lengths.to('cpu'), batch_first=True, enforce_sorted=False)\n",
    "        # LSTM需要定长序列，使用该函数将变长序列打包\n",
    "        # print(f\"经过打包为：{x_pack.size()}\")\n",
    "        hidden, (hn, cn) = self.lstm(x_pack)\n",
    "        # print(f\"经过lstm计算后为：{hn.size()}\")\n",
    "        outputs = self.output(hn[-1])\n",
    "        # print(f\"输出层输出为：{outputs.size()}\")\n",
    "        log_probs = F.log_softmax(outputs, dim = -1)\n",
    "        # print(f\"输出概率值为：{probs}\")\n",
    "        # 归一化为概率值\n",
    "        return log_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 修改collate_fn函数\n",
    "def collate_fn(examples):\n",
    "    # 从独立样本集合中构建各批次的输入输出\n",
    "    lengths = torch.tensor([len(ex[0]) for ex in examples])\n",
    "    # 获取每个序列的长度\n",
    "    inputs = [torch.tensor(ex[0]) for ex in examples]\n",
    "    # 将输入inputs定义为一个张量的列表，每一个张量为句子对应的索引值序列\n",
    "    targets = torch.tensor([ex[1] for ex in examples], dtype=torch.long)\n",
    "    # 目标targets为该批次所有样例输出结果构成的张量\n",
    "    inputs = pad_sequence(inputs, batch_first=True)\n",
    "    # 将用pad_sequence对批次类的样本进行补齐\n",
    "    return inputs, lengths, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(\n",
       "  (embedding): Embedding(21402, 128)\n",
       "  (lstm): LSTM(128, 24, batch_first=True)\n",
       "  (output): Linear(in_features=24, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 训练\n",
    "# 超参数设置\n",
    "embedding_dim = 128\n",
    "hidden_dim = 24\n",
    "batch_size = 16\n",
    "num_epoch = 10\n",
    "\n",
    "train_data, test_data, vocab = load_sentence_polarity()\n",
    "# 加载数据\n",
    "train_dataset = BowDataset(train_data)\n",
    "test_dataset = BowDataset(test_data)\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=True)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=1, collate_fn=collate_fn, shuffle=False)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = LSTM(len(vocab), embedding_dim, hidden_dim, num_class)\n",
    "model.to(device)\n",
    "# 加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 0: 100%|██████████| 500/500 [00:05<00:00, 85.99it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:343.55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 2662/2662 [00:04<00:00, 646.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC:0.58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 500/500 [00:04<00:00, 106.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:288.63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 2662/2662 [00:04<00:00, 656.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC:0.68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 500/500 [00:04<00:00, 108.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:178.49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 2662/2662 [00:04<00:00, 653.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC:0.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 500/500 [00:04<00:00, 112.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:93.57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 2662/2662 [00:04<00:00, 655.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC:0.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 500/500 [00:04<00:00, 107.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:47.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 2662/2662 [00:04<00:00, 664.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC:0.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 500/500 [00:04<00:00, 108.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:23.27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 2662/2662 [00:04<00:00, 661.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC:0.70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6: 100%|██████████| 500/500 [00:04<00:00, 107.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:12.83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 2662/2662 [00:04<00:00, 623.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC:0.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7: 100%|██████████| 500/500 [00:04<00:00, 109.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:10.88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 2662/2662 [00:04<00:00, 662.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC:0.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8: 100%|██████████| 500/500 [00:04<00:00, 110.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:7.61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 2662/2662 [00:03<00:00, 688.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC:0.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9: 100%|██████████| 500/500 [00:04<00:00, 112.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:2.80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 2662/2662 [00:04<00:00, 662.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC:0.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "nll_loss = nn.NLLLoss()\n",
    "# 负对数似然损失\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# Adam优化器\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epoch):\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_data_loader, desc=f\"Training Epoch {epoch}\"):\n",
    "        inputs, lengths, targets = [x.to(device) for x in batch]\n",
    "        # print(inputs.size())\n",
    "        log_probs = model(inputs, lengths)\n",
    "        loss = nll_loss(log_probs, targets)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Loss:{total_loss:.2f}\")\n",
    "\n",
    "    # 测试\n",
    "    acc = 0\n",
    "    for batch in tqdm(test_data_loader, desc=f\"Testing\"):\n",
    "        inputs, lengths, targets = [x.to(device) for x in batch]\n",
    "        with torch.no_grad():\n",
    "            output = model(inputs, lengths)\n",
    "            acc += (output.argmax(dim=1) == targets).sum().item()\n",
    "    print(f\"ACC:{acc / len(test_data_loader):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. 训练Transformer网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 首先实现一个位置编码层\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=512) -> None:\n",
    "        # d_model：模型计算公式中的参数\n",
    "        # dropout：辍学率\n",
    "        # max_len: 事先准备好的序列长度\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        # 生成全零的矩阵,5000*512的矩阵，5000个位置，每个位置用一个512维度向量来表示位置编码\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        # 生成位置序列，unsqueeze用于升一个维度，(5000,) -> (5000,1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        # 公式中的w_k\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        # 偶数位置编码\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        # 奇数位置编码\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)    \n",
    "        # 升维，为batch_size留出位置\n",
    "        self.register_buffer('pe', pe)\n",
    "        # 在内存中定一个常量，即将位置编码存进去\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 前向计算\n",
    "        # x:输入词向量序列\n",
    "        # print(x.size())\n",
    "        # print(self.pe.size())\n",
    "        try:\n",
    "            x = x + self.pe[:x.size(0), :]\n",
    "        except:\n",
    "            # print(x.size())\n",
    "            # print(self.pe.size())\n",
    "            pass\n",
    "            # 针对有一个输入会出现问题，将其跳过，不进行位置编码\n",
    "        # 输入词向量与位置编码相加\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一个根据序列长度生成Mask矩阵的函数\n",
    "def length_to_mask(lengths):\n",
    "    # lengths:给定序列长度\n",
    "    max_len = torch.max(lengths)\n",
    "    # print(\"maxlen\", max_len.is_cuda)\n",
    "    # print(torch.arange(max_len).is_cuda)\n",
    "    mask = torch.arange(max_len).to(\"cuda\").expand(lengths.shape[0], max_len) < lengths.unsqueeze(1)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个Transformer类\n",
    "# 此处书中代码有误，不需要hidden_dim，注意力层输入维度应该直接是词向量维度\n",
    "class Transformer(nn.Module):\n",
    "    # 基类为nn.Module\n",
    "    def __init__(self, vocab_size, embedding_dim, num_class,\n",
    "    dim_feedforward=512, num_head=2, num_layers=2, dropout=0.1, max_len=128, activation: str = \"relu\"):\n",
    "        # 构造函数\n",
    "        # vocab_size:词表大小\n",
    "        # embedding_dim：词向量维度\n",
    "        # hidden_dim：隐藏层维度\n",
    "        # num_class:多分类个数\n",
    "        # dim_feedforward：前馈网络模型的维度\n",
    "        # num_head:头数\n",
    "        # num_layers：注意力层数\n",
    "        # dropout:辍学比例\n",
    "        # max_len:序列最大长度\n",
    "        # activation:激活函数\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # 词向量层\n",
    "        self.position_embedding = PositionalEncoding(embedding_dim, dropout, max_len)\n",
    "        # 位置编码层\n",
    "        encoder_layer = nn.TransformerEncoderLayer(embedding_dim, num_head, dim_feedforward, dropout, activation)\n",
    "        # 一个encoder\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        # 注意力编码层\n",
    "        self.output = nn.Linear(embedding_dim, num_class)\n",
    "        # 输出层，线性变换\n",
    "\n",
    "    def forward(self, inputs, lengths):\n",
    "        # 前向计算函数\n",
    "        # inputs:输入\n",
    "        # lengths:打包的序列长度\n",
    "        # print(f\"输入为：{inputs.size()}\")\n",
    "        inputs = torch.transpose(inputs, 0, 1)\n",
    "        # 首先需要将输入第一维与第二维互换，适应transformer的输入\n",
    "        embeds = self.embedding(inputs)\n",
    "        # 注意这儿是词向量层，不是词袋词向量层\n",
    "        # print(f\"词向量层输出为：{embeds.size()}\")\n",
    "        embeds = self.position_embedding(embeds)\n",
    "        # 加入位置编码\n",
    "        # print(f\"位置编码层输出为：{embeds.size()}\")\n",
    "        attention_mask = length_to_mask(lengths) == False\n",
    "        # 生成mask掩码\n",
    "        # print(f\"生成mask为：{attention_mask.size()}\")\n",
    "        hidden_states = self.transformer(embeds, src_key_padding_mask = attention_mask)\n",
    "        # 用来遮蔽<PAD>以避免pad token的embedding输入\n",
    "        # print(f\"经过transformer计算后为：{hidden_states.size()}\")\n",
    "        hidden_states = hidden_states[0, :, :]\n",
    "        # 取第一个标记的输出结果作为分类层的输入\n",
    "        outputs = self.output(hidden_states)\n",
    "        # print(f\"输出层输出为：{outputs.size()}\")\n",
    "        log_probs = F.log_softmax(outputs, dim = -1)\n",
    "        # print(f\"输出概率值为：{probs}\")\n",
    "        # 归一化为概率值\n",
    "        return log_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (embedding): Embedding(21402, 128)\n",
       "  (position_embedding): PositionalEncoding()\n",
       "  (transformer): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (output): Linear(in_features=128, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 训练\n",
    "# 超参数设置\n",
    "embedding_dim = 128\n",
    "batch_size = 16\n",
    "num_epoch = 10\n",
    "num_class = 2\n",
    "\n",
    "train_data, test_data, vocab = load_sentence_polarity()\n",
    "# 加载数据\n",
    "train_dataset = BowDataset(train_data)\n",
    "test_dataset = BowDataset(test_data)\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=True)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=1, collate_fn=collate_fn, shuffle=False)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Transformer(len(vocab), embedding_dim, num_class)\n",
    "model.to(device)\n",
    "# 加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 0: 100%|██████████| 500/500 [00:07<00:00, 69.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:348.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 2662/2662 [00:07<00:00, 361.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC:0.53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 500/500 [00:06<00:00, 77.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:328.55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 2662/2662 [00:07<00:00, 352.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC:0.58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 500/500 [00:06<00:00, 74.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:315.31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 2662/2662 [00:08<00:00, 322.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC:0.59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 500/500 [00:06<00:00, 74.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:298.82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 2662/2662 [00:07<00:00, 334.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC:0.61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 500/500 [00:06<00:00, 72.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:284.84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 2662/2662 [00:07<00:00, 339.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC:0.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 500/500 [00:06<00:00, 72.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:271.36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 2662/2662 [00:07<00:00, 337.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC:0.63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6: 100%|██████████| 500/500 [00:06<00:00, 72.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:253.22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 2662/2662 [00:07<00:00, 337.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC:0.63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7: 100%|██████████| 500/500 [00:06<00:00, 73.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:240.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 2662/2662 [00:07<00:00, 333.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC:0.63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8: 100%|██████████| 500/500 [00:06<00:00, 73.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:223.48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 2662/2662 [00:07<00:00, 340.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC:0.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9: 100%|██████████| 500/500 [00:06<00:00, 74.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:269.09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 2662/2662 [00:07<00:00, 339.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC:0.59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "nll_loss = nn.NLLLoss()\n",
    "# 负对数似然损失\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# Adam优化器\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epoch):\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_data_loader, desc=f\"Training Epoch {epoch}\"):\n",
    "        inputs, lengths, targets = [x.to(device) for x in batch]\n",
    "        # print(inputs.size())\n",
    "        # print(\"inputs\", inputs.is_cuda)\n",
    "        # print(\"lengths\", lengths.is_cuda)\n",
    "        log_probs = model(inputs, lengths)\n",
    "        loss = nll_loss(log_probs, targets)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Loss:{total_loss:.2f}\")\n",
    "\n",
    "    # 测试\n",
    "    acc = 0\n",
    "    for batch in tqdm(test_data_loader, desc=f\"Testing\"):\n",
    "        inputs, lengths, targets = [x.to(device) for x in batch]\n",
    "        with torch.no_grad():\n",
    "            output = model(inputs, lengths)\n",
    "            acc += (output.argmax(dim=1) == targets).sum().item()\n",
    "    print(f\"ACC:{acc / len(test_data_loader):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. 基于循环神经网络的词性标注实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用宾州树库词性标注数据库\n",
    "from nltk.corpus import treebank\n",
    "from torch.nn.utils.rnn import pad_packed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents, postags = zip(*(zip(*sent) for sent in treebank.tagged_sents()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_treebank():\n",
    "    # 定义数据加载函数\n",
    "    sents, postags = zip(*(zip(*sent) for sent in treebank.tagged_sents()))\n",
    "    # sents存储全部经过标记化的句子\n",
    "    # postags存储每个标记对应的词性标注结果\n",
    "    vocab= Vocab.build(sents, reserved_tokens=[\"<pad>\"])\n",
    "    # 使用pad标记来补齐序列长度\n",
    "    tag_vocab = Vocab.build(postags)\n",
    "    # 将词性标注标签也映射为索引值\n",
    "    train_data = [(vocab.convert_tokens_to_ids(sentence), tag_vocab.convert_tokens_to_ids(tags))\\\n",
    "         for sentence, tags in zip(sents[:3000], postags[:3000])]\n",
    "    # 取前3000句作为训练数据，将token映射为对应的索引值\n",
    "    test_data = [(vocab.convert_tokens_to_ids(sentence), tag_vocab.convert_tokens_to_ids(tags))\\\n",
    "         for sentence, tags in zip(sents[3000:], postags[3000:])]\n",
    "    # 其余的作为测试数据\n",
    "\n",
    "    return train_data, test_data, vocab, tag_vocab\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 修改collate_fn函数\n",
    "def collate_fn(examples):\n",
    "    # 从独立样本集合中构建各批次的输入输出\n",
    "    lengths = torch.tensor([len(ex[0]) for ex in examples])\n",
    "    # 获取每个序列的长度\n",
    "    inputs = [torch.tensor(ex[0]) for ex in examples]\n",
    "    # 将输入inputs定义为一个张量的列表，每一个张量为句子对应的索引值序列\n",
    "    targets = [torch.tensor(ex[1]) for ex in examples]\n",
    "    # 目标targets为该批次所有样例输出结果构成的张量，同文本分类任务不同\n",
    "    inputs = pad_sequence(inputs, batch_first=True, padding_value=vocab[\"<pad>\"])\n",
    "    targets = pad_sequence(targets, batch_first=True, padding_value=vocab[\"<pad>\"])\n",
    "    # 将用pad_sequence对批次类的样本进行补，标签也需要补齐\n",
    "    return inputs, lengths, targets, inputs != vocab[\"<pad>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个LSTM类\n",
    "class LSTM(nn.Module):\n",
    "    # 基类为nn.Module\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_class):\n",
    "        # 构造函数\n",
    "        # vocab_size:词表大小\n",
    "        # embedding_dim：词向量维度\n",
    "        # hidden_dim：隐藏层维度\n",
    "        # num_class:多分类个数\n",
    "        super(LSTM, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # 词向量层\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first = True)\n",
    "        # lstm层\n",
    "        self.output = nn.Linear(hidden_dim, num_class)\n",
    "        # 输出层，线性变换\n",
    "\n",
    "    def forward(self, inputs, lengths):\n",
    "        # 前向计算函数\n",
    "        # inputs:输入\n",
    "        # lengths:打包的序列长度\n",
    "        # print(f\"输入为：{inputs.size()}\")\n",
    "        embeds = self.embedding(inputs)\n",
    "        # 注意这儿是词向量层，不是词袋词向量层\n",
    "        # print(f\"词向量层输出为：{embeds.size()}\")\n",
    "        x_pack = pack_padded_sequence(embeds, lengths.to('cpu'), batch_first=True, enforce_sorted=False)\n",
    "        # LSTM需要定长序列，使用该函数将变长序列打包\n",
    "        # print(f\"经过打包为：{x_pack.size()}\")\n",
    "        hidden, (hn, cn) = self.lstm(x_pack)\n",
    "        # print(f\"经过lstm计算后为：{hn.size()}\")\n",
    "        hidden, _ = pad_packed_sequence(hidden, batch_first = True)\n",
    "        # 词性标注需要再进行解包，还原成经过补齐的序列\n",
    "        # print(f\"解包之后输出为：{hidden.size()}\")\n",
    "        outputs = self.output(hidden)\n",
    "        # 在词性标注中需要使用全部的隐藏层状态\n",
    "        # print(f\"输出层输出为：{outputs.size()}\")\n",
    "        log_probs = F.log_softmax(outputs, dim = -1)\n",
    "        # print(f\"输出概率值为：{log_probs}\")\n",
    "        # 归一化为概率值\n",
    "        return log_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(\n",
       "  (embedding): Embedding(12410, 128)\n",
       "  (lstm): LSTM(128, 64, batch_first=True)\n",
       "  (output): Linear(in_features=64, out_features=47, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "# 训练\n",
    "# 超参数设置\n",
    "embedding_dim = 128\n",
    "batch_size = 16\n",
    "num_epoch = 10\n",
    "hidden_dim = 64\n",
    "\n",
    "train_data, test_data, vocab, tag_vocab = load_treebank()\n",
    "# 加载数据\n",
    "train_dataset = BowDataset(train_data)\n",
    "test_dataset = BowDataset(test_data)\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=True)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=1, collate_fn=collate_fn, shuffle=False)\n",
    "\n",
    "num_class = len(tag_vocab)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = LSTM(len(vocab), embedding_dim, hidden_dim, num_class)\n",
    "device = 'cpu'\n",
    "model.to(device)\n",
    "# 加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 0: 100%|██████████| 188/188 [00:08<00:00, 21.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:429.64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 914/914 [00:01<00:00, 511.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC:16.44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 188/188 [00:08<00:00, 21.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:212.27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 914/914 [00:01<00:00, 476.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC:18.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 188/188 [00:08<00:00, 21.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:145.63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 914/914 [00:01<00:00, 494.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC:20.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 188/188 [00:08<00:00, 21.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:107.94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 914/914 [00:02<00:00, 447.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC:21.06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 188/188 [00:09<00:00, 20.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:83.16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 914/914 [00:01<00:00, 495.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC:21.56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 188/188 [00:08<00:00, 21.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:65.19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 914/914 [00:01<00:00, 517.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC:21.83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6: 100%|██████████| 188/188 [00:08<00:00, 21.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:51.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 914/914 [00:01<00:00, 506.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC:22.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7: 100%|██████████| 188/188 [00:08<00:00, 21.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:41.47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 914/914 [00:01<00:00, 509.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC:22.18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8: 100%|██████████| 188/188 [00:08<00:00, 21.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:33.47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 914/914 [00:01<00:00, 507.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC:22.28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9: 100%|██████████| 188/188 [00:08<00:00, 22.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:27.21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 914/914 [00:01<00:00, 506.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC:22.34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "nll_loss = nn.NLLLoss()\n",
    "# 负对数似然损失\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# Adam优化器\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epoch):\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_data_loader, desc=f\"Training Epoch {epoch}\"):\n",
    "        inputs, lengths, targets, mask = [x.to(device) for x in batch]\n",
    "        # print(inputs.sizbe())\n",
    "        # print(\"inputs\", inputs.is_cuda)\n",
    "        # print(\"lengths\", lengths.is_cuda)\n",
    "        log_probs = model(inputs, lengths)\n",
    "        loss = nll_loss(log_probs[mask], targets[mask])\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Loss:{total_loss:.2f}\")\n",
    "\n",
    "    # 测试\n",
    "    acc = 0\n",
    "    for batch in tqdm(test_data_loader, desc=f\"Testing\"):\n",
    "        inputs, lengths, targets, mask = [x.to(device) for x in batch]\n",
    "        with torch.no_grad():\n",
    "            output = model(inputs, lengths)\n",
    "            acc += (output.argmax(dim=-1) == targets)[mask].sum().item()\n",
    "    print(f\"ACC:{acc / len(test_data_loader):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. 基于Transformer网络的词性标注实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个Transformer类\n",
    "# 此处书中代码有误，不需要hidden_dim，注意力层输入维度应该直接是词向量维度\n",
    "class Transformer(nn.Module):\n",
    "    # 基类为nn.Module\n",
    "    def __init__(self, vocab_size, embedding_dim, num_class,\n",
    "    dim_feedforward=512, num_head=2, num_layers=2, dropout=0.1, max_len=128, activation: str = \"relu\"):\n",
    "        # 构造函数\n",
    "        # vocab_size:词表大小\n",
    "        # embedding_dim：词向量维度\n",
    "        # hidden_dim：隐藏层维度\n",
    "        # num_class:多分类个数\n",
    "        # dim_feedforward：前馈网络模型的维度\n",
    "        # num_head:头数\n",
    "        # num_layers：注意力层数\n",
    "        # dropout:辍学比例\n",
    "        # max_len:序列最大长度\n",
    "        # activation:激活函数\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        # 词向量层\n",
    "        self.position_embedding = PositionalEncoding(embedding_dim, dropout, max_len)\n",
    "        # 位置编码层\n",
    "        encoder_layer = nn.TransformerEncoderLayer(embedding_dim, num_head, dim_feedforward, dropout, activation)\n",
    "        # 一个encoder\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        # 注意力编码层\n",
    "        self.output = nn.Linear(embedding_dim, num_class)\n",
    "        # 输出层，线性变换\n",
    "\n",
    "    def forward(self, inputs, lengths):\n",
    "        # 前向计算函数\n",
    "        # inputs:输入\n",
    "        # lengths:打包的序列长度\n",
    "        # print(f\"输入为：{inputs.size()}\")\n",
    "        inputs = torch.transpose(inputs, 0, 1)\n",
    "        # 首先需要将输入第一维与第二维互换，适应transformer的输入\n",
    "        embeds = self.embedding(inputs)\n",
    "        # 注意这儿是词向量层，不是词袋词向量层\n",
    "        # print(f\"词向量层输出为：{embeds.size()}\")\n",
    "        embeds = self.position_embedding(embeds)\n",
    "        # 加入位置编码\n",
    "        # print(f\"位置编码层输出为：{embeds.size()}\")\n",
    "        attention_mask = length_to_mask(lengths) == False\n",
    "        # 生成mask掩码\n",
    "        # print(f\"生成mask为：{attention_mask.size()}\")\n",
    "        hidden_states = self.transformer(embeds, src_key_padding_mask = attention_mask).transpose(0, 1)\n",
    "        # 用来遮蔽<PAD>以避免pad token的embedding输入\n",
    "        # print(f\"经过transformer计算后为：{hidden_states.size()}\")\n",
    "        # hidden_states = hidden_states[0, :, :]\n",
    "        # 取序列中每个输入的隐藏层作为分类层的输入\n",
    "        outputs = self.output(hidden_states)\n",
    "        # print(f\"输出层输出为：{outputs.size()}\")\n",
    "        log_probs = F.log_softmax(outputs, dim = -1)\n",
    "        # print(f\"输出概率值为：{probs}\")\n",
    "        # 归一化为概率值\n",
    "        return log_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (embedding): Embedding(12410, 128)\n",
       "  (position_embedding): PositionalEncoding()\n",
       "  (transformer): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (output): Linear(in_features=128, out_features=47, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 训练\n",
    "# 超参数设置\n",
    "embedding_dim = 128\n",
    "batch_size = 16\n",
    "num_epoch = 10\n",
    "\n",
    "train_data, test_data, vocab, tag_vocab = load_treebank()\n",
    "# 加载数据\n",
    "# 加载数据\n",
    "train_dataset = BowDataset(train_data)\n",
    "test_dataset = BowDataset(test_data)\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate_fn, shuffle=True)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=1, collate_fn=collate_fn, shuffle=False)\n",
    "num_class = len(tag_vocab)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Transformer(len(vocab), embedding_dim, num_class)\n",
    "model.to(device)\n",
    "# 加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 0: 100%|██████████| 188/188 [00:05<00:00, 35.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:269.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 914/914 [00:04<00:00, 185.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC:18.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 1: 100%|██████████| 188/188 [00:05<00:00, 33.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:143.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 914/914 [00:05<00:00, 182.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC:20.28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 2: 100%|██████████| 188/188 [00:05<00:00, 34.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:96.47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 914/914 [00:04<00:00, 185.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC:21.13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 3: 100%|██████████| 188/188 [00:05<00:00, 34.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:68.14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 914/914 [00:04<00:00, 188.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC:21.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 4: 100%|██████████| 188/188 [00:05<00:00, 36.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:50.45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 914/914 [00:04<00:00, 191.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC:21.82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 5: 100%|██████████| 188/188 [00:05<00:00, 33.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:37.56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 914/914 [00:04<00:00, 187.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC:21.76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 6: 100%|██████████| 188/188 [00:05<00:00, 35.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:28.68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 914/914 [00:05<00:00, 171.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC:21.94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 7: 100%|██████████| 188/188 [00:05<00:00, 34.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:23.09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 914/914 [00:05<00:00, 180.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC:21.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 8: 100%|██████████| 188/188 [00:05<00:00, 32.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:19.21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 914/914 [00:05<00:00, 176.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC:22.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 9: 100%|██████████| 188/188 [00:05<00:00, 32.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:16.77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 914/914 [00:05<00:00, 179.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC:21.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "nll_loss = nn.NLLLoss()\n",
    "# 负对数似然损失\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "# Adam优化器\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epoch):\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_data_loader, desc=f\"Training Epoch {epoch}\"):\n",
    "        inputs, lengths, targets, mask = [x.to(device) for x in batch]\n",
    "        # print(inputs.sizbe())\n",
    "        # print(\"inputs\", inputs.is_cuda)\n",
    "        # print(\"lengths\", lengths.is_cuda)\n",
    "        log_probs = model(inputs, lengths)\n",
    "        loss = nll_loss(log_probs[mask], targets[mask])\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Loss:{total_loss:.2f}\")\n",
    "\n",
    "    # 测试\n",
    "    acc = 0\n",
    "    for batch in tqdm(test_data_loader, desc=f\"Testing\"):\n",
    "        inputs, lengths, targets, mask = [x.to(device) for x in batch]\n",
    "        with torch.no_grad():\n",
    "            output = model(inputs, lengths)\n",
    "            acc += (output.argmax(dim=-1) == targets)[mask].sum().item()\n",
    "    print(f\"ACC:{acc / len(test_data_loader):.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('env_for_gpu')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b125423cf5c6421dfa630e1e0530026ff8478baf79673e49ec884511905104d1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
